\documentclass[12pt]{article}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{microtype}
\usepackage[a4paper,margin=2cm]{geometry}
\usepackage{vwcol} 
\usepackage{lipsum,multicol}
\usepackage[colorlinks]{hyperref} 
\usepackage{caption}
\usepackage{pgfplots}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{tikz}
\usetikzlibrary{positioning}

% Expectation %
\setlength\parindent{0pt}
\DeclareMathOperator{\E}{\mathbb{E}}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}

\title{CS3243 Introduction to Artificial Intelligence}

\date{AY2019/20 Semester 2}
\author{Choong Wey Yeh}

\begin{document}

\maketitle

\section{Uninformed Search}

In this section, we will be discussing some of the basic uninformed search algorithms where these algorithms do not make use of certain heuristics to optimize their search strategy. Most of the time, we will observe that these basic uninformed search algorithms perform complete searches; that is they iterate over all the possible elements in the solution space before terminating.\\

For this section, we will be working with:
\begin{itemize}
\item \textbf{Fully-Observable Environment:} Each item in the solution space is known to the algorithm; i.e there are no hidden states
\item \textbf{Deterministic Setting:} No uncertainty/probabilistic notions involved in state change
\item \textbf{Discrete Environment:} States consist of discrete variables instead of continuous uncountable ones
\end{itemize}

Some of these examples include winning a sequence of moves in a single-player game, assembly of machine parts and boolean satisfiability(SAT) problem. Essentially, we can formulate each of these examples as a search problem where we iterate over the possible solutions. For example, in the SAT problem, we can perform a complete uninformed search over each combination of variables such that the boolean formula is satisfied. Of course, we will eventually see that this is not an efficient(nor practical) way to attain a solution for various problems.

\subsection{Problem Formulation}

Formally, we can define the basic elements of a problem definition using \textbf{states}, which are the current configurations for the problem(e.g the variable values for a SAT problem), and \textbf{actions} which causes a transition of one state to another via changing configurations(setting a boolean variable from true to false). When considering the configurations for state and actions, most of the factors are unnecessary and may be \textbf{abstracted out}. For the route planning problem in particular, we are concerned with how route the driver takes to get to a location, but \textit{not exactly how he drives there}, such as which lane or which car he uses. In this case, such information may be left out of the problem definition.\\

Additionally, we also have an \textbf{initial state}, which are the parameters that the algorithm starts with to search for the solution. This initial state can be default setting of certain parameters, or random initialization. But, we also need the following definitions:

\subsubsection{Goal State}

The \textit{goal state} is defined as the state that determines the termination of the algorithm as it has reached the objective of the solving the problem. When the search algorithm reaches a state that is deemed as the goal state, it will stop and return it's current solution. The goal state can be defined in various ways:

\begin{itemize}
\item \textbf{Explicit Set} of goal states, which can be the configurations that the algorithm permutates through. Often for problems with large solution spaces, this is not very practical as the set could be very large. 
\item \textbf{Implicit Function} $f(x)$ that evaluates the current configuration of the algorithm and outputs if the configuration is indeed a goal state or not, where $x$ is the current configuration used as the input.
\end{itemize}

Ideally, we want the evaluation of whether the current configuration is the goal state or not to be \textit{efficient}, as this evaluation function is called every single time the algorithm changes states, and could be called a large number of times(e.g if boolean formula in SAT is very long). Hence, an inefficient evaluation function could be computationally expensive and slow down the search process immensely.

\subsubsection{Path Cost}

When we are discussing about a search problem, very often each transition from one state to another incurs a certain cost or penalty, such in the route planning problem where there is a cost incurred moving from a location to another. Then, a sequence of state transitions. which are viewed as actions, is a \textit{search path} taken by the algorithm to find the optimal solution by viewing the possible actions that can be taken by the algorithm now as a decision tree like structure. We denote the cost of an action, such as a transition from one state to another as the following function:

\begin{equation*}
c(s, a, s')
\end{equation*}

where an action of $a$ causes the algorithm to transition from state $s$ to $s'$. Here, if $a_i$ is the $i^{th}$ action taken by the algorithm, then the sequence of actions $a_1, a_2, ..., a_n$ is the search path of the algorithm. If this sequence of actions results in the final state being the goal state, then it is a solution to the problem. Note that the solution \textbf{may not be unique}.

\subsubsection{Performance}

Not required for the problem formulation, but it is also eventually necessary to make sure that our algorithm works, and is efficient. In particular, we need to consider:

\begin{itemize}
\item \textbf{Termination:} We want for our solution to eventually find a solution. If the algorithm does not terminate at all, then we may never get an answer as the algorithm continues running
\item \textbf{Optimality:} We want a feasible solution, but above that we want to find the optimal feasible solution. That is, for a solution $y$, the solution is optimal if for all other feasible solutions $y'$, $c(y) \leq c(y')$. That is, the cost of our solution is minimal
\item \textbf{Efficiency:} defined as the search cost as well, is the time and space complexity of our algorithm with respect to the size of the solution space
\end{itemize}

\subsection{Search Strategies}

\subsubsection{Search Representation}

We shall represent the states and actions in our search problem as nodes and edges in a tree or graph, such as in \textbf{tree search} and \textbf{graph search}. Each node represents a state from the state space, and the edge connecting node $s$ and $s'$ represents a transition from state $s$ to $s'$.\\

Additionally, we also introduce the definition of a \textbf{frontier}. After we have reached a node $x$, the other nodes connected to $x$ are divided into those that are visited, and those that are not. The nodes that are visible from node $x$, but are not yet visited and can be expanded for exploration. These nodes form the frontier. Our search algorithm makes use of the frontier as a pool of nodes that are available to be visited next down it's search path. In tree search, we are allowed to repeat the nodes that we have visited(e.g backtracking), but in graph search we no longer return to a node once we have visited it.

\subsubsection{Search Performance}

Previously, we mentioned that the following criteria will be used to judge the performance of our algorithm:

\begin{itemize}
\item \textbf{Completeness:} Is the algorithm guaranteed to find a solution(if the solution exists)?
\item \textbf{Time Complexity:} How long does the algorithm take to find a solution in the worst case?
\item \textbf{Space Complexity:} How much space is required for the algorithm to perform the search?	
\item \textbf{Optimality:} Does the algorithm find the highest quality solution in the presence of multiple feasible solutions?
\end{itemize}

We shall explicitly evaluate the upcoming algorithms using the following parameters:

\begin{itemize}
\item \textbf{Branching Factor $b$:} The maximum number of children, or successors of a node in a search tree of graph
\item \textbf{Depth of Shallowest Goal Node $d$:} Height of the tree before the shallowest goal node is reached
\item \textbf{Maximum Depth $m$:} Maximum depth of the search tree
\end{itemize}

Instead of the standard time complexity evaluation using $V$ and $E$ as the number of nodes and edges, due to the large size of search problems.

\subsubsection{Breadth First Search}

The basic idea of BFS is that at any point of time in the search, the shallowest nodes are explored first. In particular, if the frontier consists of nodes from depth $d, d + 1,...$ then all nodes at depth $d$ are visited first before $d + 1$. BFS can easily be implemented by using a queuing function that accepts the queued neighbours of visited nodes, and outputs them in the order that it is received where newly generated states are polled last.\\

\begin{itemize}
\item \textbf{Completeness:} BFS is a very systematic strategy, where it observes all nodes of depth 1, then depth 2 and so on. Hence, at depth $d$, BFS would have finished visiting all nodes of depth $d-1$ and lesser. By the time BFS reaches depth $m + 1$, it would have visited all nodes of depth $m$ and before, which is all the nodes in the tree. Hence if a solution exists, it is sure to have found one.

\item \textbf{Time Complexity:} Consider the case where each node in the search tree has the maximum number of children $b$. Then, the first level of the search tree would generate $b$ children, and each of these children continues to generate another $b$ and so on. Then, the maximum number of nodes that the algorithm has to visit would be:
\begin{equation*}
1 + b + b^2 + ... + b^d = O(b^d)
\end{equation*}

where at the $i^{th}$ level, the search tree has $b^i$ children. Since the algorithm terminates when it has found the first goal node, if the shallowest goal node is at level $d$ then BFS would have eventually found it and does not continue past depth $d$.\\

While the time complexity is exponential in the input, BFS can be useful if heuristic says that the search tree is wide but shallow, or if the goal node is within a certain depth from the root.

\item \textbf{Space Complexity:} Similar to the time complexity, assuming the worst case scenario of a complete tree with branching factor $b$, then there are $O(b^d)$ children, and hence $O(b^d)$ amount of memory required to store all these children.

\item \textbf{Optimality:} If we consider a special case of search where the search step cost is 1, then the optimal solution for this search problem would be a feasible solution with the least number of nodes in the search path, which would be the shallowest goal node which has the lowest possible depth. Since BFS traverses the tree level by level, it would find the shallowest goal node first among all other goal nodes, which is the optimal solution.\\

However, most of the time the step cost is not unit, and in that case BFS does not usually find the optimal solution. This is because it finds the shallowest goal node, but it is not necessarily the \textit{least cost} path solution. We shall try to rectify this issue in the next search algorithm.
\end{itemize}

\subsubsection{Uniformed Cost Search}

UCS modifies the BFs strategy by always expanding the lowest cost node in the frontier, measured by a path cost function $g(n)$ as the cost of the path from the root to the current node $n$. It is trivial to see that BFS is UCS with path cost function $g(n) = depth(n)$. In implementation, we modify from BFS slightly again by changing the queue used a priority queue, or a heap data structure that emphasises cheapest paths.

\begin{itemize}
\item \textbf{Completeness:} UCS is complete, under the caveat that the minimum step cost is $\geq \epsilon$, where $\epsilon$ is a non-negative number. If there is no bound on the step cost, then we could have cases of negative costs or exponentially decreasing costs. In this case, the algorithm could be stuck without termination.

\item \textbf{Optimality:} Under the same conditions as completeness, the solution is optimal provided step costs are lower bounded by a non-negative number. This is because the first solution found by the algorithm is guaranteed to be the cheapest cost solution. If there were a cheapest cost solution, the algorithm would have found it first.\\

Note the observation that because step cost $\geq \epsilon$, the step cost must never decrease. In other words, $g(successor(n)) > g(n)$. However, if we lift the lower bound on step cost, then the search problem is reduced to that of BFS, because a long and expensive path may become the optimal by running into a node with extremely high negative cost. As a result, we need to search all possible paths to obtain the optimal.

\item \textbf{Time Complexity:} Remember that in the queue, we keep at most $b^i$ nodes in the queue if we are at depth $i$. Since the step cost $\geq \epsilon$, when the algorithm goes down a search path from depth $i$ to $i+1$, the overall current cost increases by at least $\epsilon$. Consequently, if the optimal path cost is $C^*$, then there are at most $\frac{C^*}{\epsilon} + 1$ nodes in the search path since the cost increases in sequence of $0, \epsilon, 2\epsilon,...,  \frac{C^*}{\epsilon}$. This also means the maximum depth the algorithm will go is at most $\frac{C^*}{\epsilon} + 1$, and hence the maximum number of nodes that it will visit is $b^{\frac{C^*}{\epsilon} + 1}$

\item \textbf{Space Complexity:} Similarly, since the maximum depth is $\frac{C^*}{\epsilon} + 1$, the maximum number of nodes that will be stored in the heap will be at most $b^{\frac{C^*}{\epsilon} + 1}$.
\end{itemize}

\subsubsection{Depth-First Search}

DFS expands one of the nodes the deepest level of the tree during the search instead of covering all nodes in a level. Only when the search hits the most bottom of the search path then does it go back to a shallower level and expand the nodes at those levels. This is good for search problems where it is beneficial to explore deeper rather than breadth. Implementation wise, DFS uses a stack instead of a queue to pick the next node.


\begin{itemize}
\item \textbf{Completeness:} Similar to BFS, DFS is complete as it eventually expands all nodes in a frontier and explore all possible paths, given that the depth of the search tree is \textit{finite}. For trees with infinite tree depths, then the algorithm may never terminate. 

\item \textbf{Optimality:} Like BFS, DFS is not optimal for the same reason that it does not use any heuristic on path cost at all.

\item \textbf{Time Complexity:} Assuming finite search tree height of $m$, let us consider the worst case that the optimal solution is a search path of length $m$ and that it is the rightmost path. Then, notice that because of the way DFS explores paths, DFS will exhaust all other possible paths before obtaining the optimal by depth-first traversal. If all nodes have the maximum successor branching of $b$, then that means DFS will go through all combinations of $b^m$ search paths in the worst case.

\item \textbf{Space Complexity:} DFS has a better space complexity than BFS of $O(bm)$. Each time the algorithm expands a node, it pushes all the successors of that node into the stack, which is at most $b$. Since the maximum depth, and the length of a search path is $m$, and DFS fully explores and exhaust a search path before it expands another node, the maximum number of items pushed on the stack is $bm$. \\

In fact we can continue to improve this space complexity further. Notice when we push the successors onto the stack, we pick one successor to continue exploring down. In this process, other successors are not used until all the possible paths following the picked successor is exhausted. In that case, we merely have to push the \textit{current} successor onto the stack to be tracked without the other successors. The algorithm can then backtrack and then pick another successor to expand along later in the search. Then, the space complexity is equivalent to the number of successors picked in a path, which is also equivalent to the maximum path length that is $O(m)$.
\end{itemize}

\subsubsection{Depth-Limited Search \& Iterative Deepening Search}

We have seen that for trees with infinitely long tree heights, then the algorithm could potentially run forever. If we knew that we only had limited time to run our search, or that the goal node is within a certain height, then we could limit the maximum height that DFS runs to. That is, we can force our algorithm to terminate once there are no longer any more search paths with length $l \leq l*$ where $l*$ is the maximum height specified.\\

However, note that if we do so, then our DLS algorithm is \textbf{not complete}. Because it only considers search paths of length $l^*$ and lesser, any search paths with longer lengths is ignored. If the goal node is of length $\geq l*$, then our algorithm would never return a solution.\\

To address this, we introduce an additional condition: To perform DLS \textit{until a solution is found}. This guarantees that the algorithm is complete and will return a solution to us. Additionally, this algorithm is similar to BFS in the sense that the search deepens level by level, like how BFS expands nodes in a depth first.

\begin{algorithm}
\caption{Iterative Deepening Search}\label{euclid}
\begin{algorithmic}[1]
\For{$depth = 0 \rightarrow \infty$}
	\State \textit{result} $\leftarrow$ DEPTH-LIMITED-SEARCH(\textit{problem, depth})
	\If{$result \neq $ cutoff}
		\State \textbf{return} \textit{result}
	\EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

This allows us to maintain the space complexity benefit of DFS over BFS, but also allows us to address the problem of large state space and unknown tree depth(which is often most of the time).\\

However, now we have another downside: we essentially repeat searches over subtrees of the search tree. We first perform DLS with depth $d$, then $d + 1$ and so on. Notice that the search space on the search tree limited with depth $d$, is a subspace of the search space on a tree with depth $d + k$. Hence, we are repeating some of the search from the previous iteration of IDS. In particular, the number of nodes expanded by DLS on depth $d$ is:

\begin{equation*}
1 + b + b^2 + ... + b^{d-1} + b^d
\end{equation*}

and the number of nodes expanded by IDS is:

\begin{equation*}
(d+1)1 + (d)b + (d-1)b^2 + ... + 2b^{d-1} + 1b^d
\end{equation*}

Notice that of all the subtrees of different depths of the search tree, IDS explores subtrees of shallowest heights the most, while the bottom-most parts of the trees is explored lesser. This is because of the fact that in every iteration of IDS, it performs DLS restarting from the root again. Hence the initial parts of the search tree are traversed more often. \\

In particular, the further down the tree the search goes, the number of successors increases, perhaps exponentially. Since these parts of the tree are explored much lesser than the top, which has much lesser nodes, in perspective the search is repeated on a very small subset of the nodes of the entire search space considered, and hence the overhead in practice is actually within accepted bounds.

\paragraph{Remarks}
\begin{itemize}
\item If $b$ is not finite, then all the uninformed algorithms we have discussed are not complete, since they will never be able to exhaust fully nodes of any depth
\item If $b$ is finite, then BFS and IDS is complete, since IDS iteratively increases the depth similar to BFS. Additionally, if the step cost is lower bounded by $\epsilon$, then UCS is also complete
\item If $m$ is finite, then DFS is complete, otherwise it may continually run forever down an infinite search path
\item if the step cost is unit(i.e = 1), then UCS is reduced to BFS, and BFS and IDS are optimal because the step cost is equivalent to the number of nodes in search path
\end{itemize}
\pagebreak

\section{Informed Search}

Previously, we have seen search strategies that makes use of the problem definition, such as the states and actions, to perform a general search action. This usually results in some form of queuing function that determines which node to expand next deterministically. However, more often than not we do not have enough information to full model the problem to determine the full cost. For example,

\begin{itemize}
\item Depth of search tree or size of solution space could be possibly infinite
\item Repeated states could be present
\item Certain constraints could be hard to model, or cause problems to be difficult to solve
\end{itemize}

If we use uninformed search strategies for these problems, optimality and completeness may not be guaranteed, in addition to the exponential time search efficiency. In this case, we can introduce additional information or heuristics that can be exploited in order to improve our search strategy. To do this, we introduce an evaluation function $f(n): X \rightarrow R$ which returns a number indicating the desirability of the node to expand next. This could be in terms of shortest path cost for example. The evaluation function that we define here serves to approximate the cost estimate from node to node in problems where full information is not present for us to perform uninformed search.

\subsection{Best-First Search}

In best-first search, we make use of the evaluation function by expanding the node that has the most desirable evaluation function score, by ordering the nodes by their evaluation function values. If the evaluation function is able to perfectly reflect the information stated in the problem definition, that is if the evaluation function is a true one, then expanding the node by the evaluation function score will be the optimal path and indeed be the best node to expand at the current point in time. However, this is often not the case and the evaluation function will reflect the node that \textit{appears} to be the best node to expand in that point in time.\\

\begin{algorithm}
\caption{Best-First Search}\label{euclid}
\begin{algorithmic}[1]
\State $result \leftarrow$ next node ordered by $f(n)$
\If{$result == goal\ state$}
	\State return $search\ path$
\Else
	\State goto $1$
\EndIf
\end{algorithmic}
\end{algorithm}

\subsubsection{Greedy Best-First Search}

A good Best-First Search strategy is to \textit{minimize estimated cost to reach the goal}. That is, we define a \textbf{heuristic function} $h(n)$ that estimates the cost from a node to the goal node. Often this function provides not an exact formulation of the actual cost from a node to the goal node, but a good enough estimate of this cost. Then, we set:

\begin{itemize}
\item $f(n) = h(n)$ and use the heuristic function to expand the next node
\item $h(t) = 0$ where $t$ is the goal node, as the distance from the goal node to itself is clearly 0
\end{itemize}

\subsubsection{Performance Measure}

\begin{itemize}
\item \textbf{Completeness:} Similar to the uninformed search functions, Best-First Search also makes use of a queuing function, albeit using an evaluation function. This means that every node will eventually be added into the queue, and expanded until the solution is found if $b$ is finite. 

\item \textbf{Optimality:} However, it is easy to see that Best-First Search \textbf{is not optimal}. Notice that Best-First Search aims to obtain low cost solutions by using the estimated cost from the node in the frontier to the goal node. However, \textit{it does not make use of past information of the cost of the path travelled so far}. Best-First Search focuses on greedily reducing the current estimated remaining cost to the goal, and completely ignores the overall path cost to reach the goal node.\\

In order to focus the search on lowest cost paths toward the goal, our evaluation function must incorporate the \textit{cost of the path} from a state to the goal state, instead of ignoring the path of the cost so far to the current node.

\item \textbf{Time Complexity:} In the worst case where the path containing the least cost weights has length of that of the height of the search tree, then the search will incur runtime exponential in the order of the branching factor $O(b^m)$

\item \textbf{Space Complexity:} Similar to the time complexity, in the worst case we may maintain $b^m$ elements in the queue for a search tree with branching factor of $b$ and a search tree depth of $m$.
\end{itemize}

Apart from in-optimality issues due to ignoring of overall path cost, Best-First Search also has various other issues:

\begin{itemize}
\item \textbf{Susceptible to False Starts:} Because the search strategy only minimizes the current remaining estimated cost, it could be possible for the algorithm to pick the shortest node that leads to a dead end, resulting in unnecessary nodes being expanded.
\item \textbf{Repeated States:} Furthermore, if we allow repeated states such as in Graph Search, then it could be possible for the algorithm to oscillate between 2 visited states if the estimated cost between the two is the cheapest among those in the frontier.
\item \textbf{Infinite Search Path Length:} Similar to Depth-First Search, if the search tree path has infinite length, then it is possible for the algorithm to keep searching down that path and never terminate
\end{itemize}

\subsection{A* Search}

We have seen that minimizes the estimated cost to the goal using the heuristic function $h(n)$ in a bout to reduce search cost, but it is not optimal. On the other hand, Uniformed-Cost Search(UCS) reduces the cost of the path so far to the goal, but it is inefficient due to it's exponential time complexity. But we can combine both strategies to obtain a search strategy with the advantages of both. We do this by summing up the the evaluation functions of both search algorithms where:

\begin{itemize}
\item $g(n)$ is the cost function from UCS that measures the cost from the start node to the current node $n$
\item $h(n)$ is the cost estimate heuristic from Best-First Search that gives an estimate of the remaining cost from the node $n$ to the goal node
\item $f(n) = g(n) + h(n)$ is the combined heuristic function that estimates the cost of the cheapest path from the start node to the goal node by expanding node $n$ from the current node.
\end{itemize}

Hence if we are aiming to minimize the overall cost of the search path, then minimizing $f(n)$ is a reasonable strategy. In fact, we will soon prove that given the following restriction on $h(n)$, then the search strategy is complete and optimal:

\begin{theorem}
If $h(n)$ is an \textbf{admissible heuristic}, then A* Search using Tree Search is optimal
\end{theorem}

\begin{definition}
A heuristic function $h(n)$ is an \textbf{admissible heuristic}, if $\forall n, h(n) \leq h^*(n)$ where $h^*(n)$ is the optimal true cost to reach the goal state from node $n$. 
\end{definition}

That is, the algorithm \textit{never overestimates the true cost of the path from node $n$ to the goal}.\\

Furthermore, we shall assume the \textbf{monotonic} property for the path costs for A* search that, all path costs are non-decreasing. That is for the path containing the node $n$ to the goal node $t*$, $f(n) \leq f(t*)$. In particular we say that the heuristic function is \textbf{consistent}.

\begin{definition}
A heuristic function $h(n)$ is \textbf{consistent}, if $\forall n$, and it's successors $n'$, $h(n) \leq c(n, n') + h(n')$.
\end{definition}

In some cases, we also call this heuristic function metric because it obeys the triangle inequality. A result from this definition is that path costs are non-decreasing, or monotonic. This can be seen via the following equation:

\begin{equation*}
\begin{aligned}
f(n') & = g(n') + h(n') \\
& = g(n) + c(n, n') + h(n') \\
& \geq g(n) + h(n) = f(n)
\end{aligned}
\end{equation*}

In particular, the following lemma holds:

\begin{lemma}
If a heuristic is consistent, then it is also admissible
\end{lemma}

A simple proof of this is that, we can take any node $n$, then we see that:

\begin{equation*}
\begin{aligned}
h(n) & \leq c(n, n') + h(n')\\
& \leq c(n, n') + c(n', n'') + h(n'') \\
& ...\\
& \leq c(n, n') + c(n', n'') + ... + c(n^{t-1}, t) + h(t) \\
& \leq c(n, n') + c(n', n'') + ... + c(n^{t-1}, t) (\text{remember $h(t) = 0$})\\
& \leq c(n, t)
\end{aligned}
\end{equation*}

If the path costs for a problem is non-monotonic, then we can use the following function to enforce monotonicity using the following $pathmax$ function:

\begin{equation*}
f(n') = max(f(n), g(n') + h(n'))
\end{equation*}

By the monotonicity property of the path costs, that means that A* will expand all nodes $n$ with evaluation function costs $f(n) < f*$ where $f*$ is the cost of the optimal solution path. This is similar to the BFS behaviour where A* expands nodes layer by layer, where each layer is a contour represented by it's evaluation function cost.

\subsubsection{Proof of Optimality of A* Search}

Let the optimal goal state in the search tree be $t*$, and a suboptimal goal state be $t$. Then, $f(t) = g(t) \leq f(t*) = g(t*)$. Let us then assume then A* algorithm has picked $t$ as the solution and terminated. We shall see that this is not possible.\\

Let $n$ be an unexpanded node on the path to the optimal goal state $t*$. Such a node exists because if it does not, that means that the algorithm that already expanded all nodes on the path to $t*$, and hence found the optimal goal state. Additionally, let it be such that 

\begin{equation*}
f(t) \leq f(n)
\end{equation*}

such that $t$ is expanded before $n$, which can be rewritten as:

 \begin{equation*}
 \begin{aligned}
f(t) \leq f(n) & = g(t) + h(t) \leq g(n) + h(n)\\
&= g(t) \leq g(n) + h(n)
\end{aligned}
\end{equation*}

since $h(t) = 0$ as $t$ is a goal state. Then, because path costs are non-decreasing, that means that:

 \begin{equation*}
 \begin{aligned}
g(t) & \leq g(n) + h(n) \\
& = f(n) \\
& \leq f(t*) \\
& = g(t*) + h(t*) = g(t*) (\text{since $h(t*) = 0$})
\end{aligned}
\end{equation*}

Which means that the path cost of reaching the suboptimal goal state $t$ is lesser than that of the optimal goal state $t*$, which contradicts our initial assumption that $f(t) = g(t) \leq f(t*) = g(t*)$. Hence, we have proven that A* search is guaranteed to find the optimal goal state. Intuitively, we can see that this is true because $f(t) \leq f(t*) = g(t*) = g(n) + h*(n)  \leq f(n) = g(n) + h(n)$ for any node $n$ that is on the optimal path from the start node to the goal node due to non-decreasing costs, and by the admissible heuristic that $h(n) \leq h*(n)$. Moreover, it is also easy to see that if we have a heuristic that \textit{overestimates} the path cost to the goal node, then the algorithm might evaluate $f(n) > f(t)$ and eventually find the suboptimal goal first.\\

Additionally,  a stronger claim is that when a node $n$ is selected for expanding, that means that \textit{the path leading up to $n$ is the shortest path found}. To see why this is so, we shall assume that the algorithm expands node $n$ via a suboptimal path with last node $q_2$ instead of a optimal path with last node $q_1$. We know that since $n$ was expanded before $q_1$, that means that $f(q_2) \leq f(n) \leq f(q_1)$. But this contradicts the optimality of the optimal path with cost $f(q_1) < f(q_2)$, and the non-decreasing property since $q_1$ is on the path to $n$ and hence $f(q_1) < f(n)$.\\

In particular, it is also proven that A* is \textbf{optimally efficient}. That is, any other search algorithm that uses an admissible heuristic that finds the optimal goal state \textit{does not expand less nodes} than A*. This also means any algorithm that does not expand nodes $n$ with $f(n) 
\leq f(t*)$ run the risk of non-completeness and not finding the optimal goal state.

\subsubsection{Proof of Completeness of A* Search}

Recall that we mentioned that path costs are non-decreasing along the path. We can visualize the search landscape of the search tree in contours, where each contour is in order of increasing path costs. We can see that A* Search eventually expands all nodes in a contour before moving onto the next. In particularly, A* search moves in increasing order of contours using the evaluation function. This means that A* search will expand up the contours, before it eventually reaches the optimal shallowest goal node.\\

However, this is only true under 2 conditions:
\begin{itemize}
\item If there are finite number of nodes with evaluation function costs $f(n) < f*$
\item If the branching factor $b$ in the search tree is finite
\end{itemize}

Similar to problems with uninformed search, we can see that if the above 2 conditions do not hold, it is possible for the algorithm to be stuck within a contour and never terminate.

\subsubsection{Time and Space Complexity of A* Search}

While A* Search is optimally efficient, it still incurs time complexity exponential in the base $b$, to the power of the error incurred in the estimation of the heuristic function resulting in $O(b^{h^*(n) - h(n)})$, unless the error grows no faster than the logarithm of the actual path cost(proof omitted):

\begin{equation*}
|h^*(n) - h(n)| \leq O(\log h^*(n))
\end{equation*}

In particular, we wish to use dominant heuristic functions over a set of possible $h(n)$

\begin{definition}
For 2 heuristic functions $h_1(n), h_2(n)$ that are admissible, if $\forall n, h_1(n) \geq h_2(n)$, then $h_1(n)\ \textbf{dominates}\ h_2(n)$
\end{definition}

This also means that $h_1(n)$ incurs lower search costs, because since both functions are admissible, values for $h_1(n)$ are closer to the true cost function $h^*(n)$ than $h_2(n)$ is.\\

This means that A* is greatly affected by the error incurred by the heuristic function estimation. However, computational time is not the biggest drawback. A* incurs a space complexity of $O(b^m)$ by maintaining nodes generated in the frontier in memory. More often than not, A* Search will run out of space before it finishes running.

\subsection{Deriving Admissible Heuristics}

Often we have to invent our own heuristic functions in order to tackle a search problem. One of the ways that we can do this is by \textbf{relaxing} the constraints to the problem. This means removing some of the restrictions placed on the problem itself, leading to special cases of the problem that can be solved using specific derivable heuristics. It is often that \textit{optimal solutions to a relaxed problem is often a good heuristic to solving the original}. By relaxing a problem, we can develop a set of admissible heuristic functions $\{h_1, h_2,..., h_n\}$ that provides optimal or near optimal solutions to specific configurations to the search problem, which can be used to evaluate different scenarios.\\

With a set of specific heuristic functions, it follows that each of these heuristics that developed to solve a specific scenario, and not a general best heuristic for the best solution itself. If we pick one heuristic over another, it could be that the heuristic does optimally in one scenario, but very bad in another. Hence instead of picking, we can make use of the dominant property to pick heuristic functions at the current state:

\begin{equation*}
h(n) = \max (h_1, h_2, ..., h_n)
\end{equation*}

Which means that always have the best option of a heuristic at any state in the search. Using the dominant property, as well as the fact that the heuristics are admissible, guarantees the best choice of heuristics among all we have by picking the heuristic closest to the upper bound of the true cost.\\

Other methods of derivation of heuristics involve statistical learning and feature evaluation as well, which allows the algorithm to derive importance of various features of heuristics in different situations by itself.

\pagebreak
\section{Local Search}

In some problems, the state description contains all the information about the problem that we need to make informed searches, and the path in which we take during our search may not be so useful anymore. We only require that the goal state be reached. In this case, local search, or \textbf{iterative improvement} algorithms may provide a practical approach that does not require searching over search paths. More often than not, it is often a good idea to \textit{start with a complete configuration of the problem, and make gradual improvements to the solution}. We can easily see that such algorithms search a smaller portion of the search space while still being able to provide reasonable solutions to the problem. Local search algorithms do not look far into a search path, and usually only considers neighbours within close proximity.

\subsection{Hill-Climbing Search}

The hill-climbing algorithm is one that continually moves the current position in the direction of an increasing objective value. The algorithm always aims to make an improvement through a movement to it's surrounding neighbours if possible. Variants of this algorithm in other problems such as convex optimization may also be known as  gradient descent.\\

\begin{algorithm}
\caption{Hill-Climbing Search}\label{euclid}
\begin{algorithmic}[1]
\State $current \leftarrow problem.$initial-state
\Loop
	\State $neighbour \leftarrow$ highest-valued successor of $current$
	\If{neighbour.Value $\leq$ current.Value}
		\State \Return current.State
	\Else 
		\State $current \leftarrow neighbour$
	\EndIf
\EndLoop
\end{algorithmic}
\end{algorithm}

Note that unlike the previous algorithms that we have seen, the Hill-Climbing search algorithm does not maintain any form of search tree or graph, but only the current state, it's neighbour states and their evaluations. This cuts down the need for large memory requirements immediately.\\

However, the Hill-Climbing search paradigm does suffer from certain drawbacks:

\begin{itemize}
\item \textbf{Local Maxima:} The algorithm may run into a point where the current state is a peak among all other neighbours, and returns this state. That is, the algorithm is guaranteed to return a \textit{maxima}, but it could be a local maxima and be maximal with respect to the neighbours, but not the entire state space and be a \textit{global maxima}. The algorithm does not know this and only recognises the local maxima as the highest value, since it does not look past the neighbours of the current state. In many cases, the local maxima returned by the algorithm may be far from satisfactory compared to the global maxima.

\item \textbf{Plateau:} This is an area of the state space where the value does not change from current to neighbouring states. In this case, the algorithm does not know for sure which move to take, and either uses heuristics to decide or makes a random walk.
\end{itemize}

The drawbacks that Hill-Climbing search has points to the fact that once the algorithm is unable to make any further improvement, it returns the current best solution without checking other peaks in the state space, and most of the time the current highest returned is a local maxima and not the optimal solution to the problem. That is, Hill-Climbing fully uses exploitation, but not exploration in it's search. We can combat this in various ways depending on the problem, but the most commonly known is \textbf{Random-Restart Hill-Climbing} by restarting the algorithm initialized to different starting states in a bout to obtain a higher maxima.

\section{Adversarial Search}

\end{document}