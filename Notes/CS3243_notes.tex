\documentclass[12pt]{article}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{microtype}
\usepackage[a4paper,margin=2cm]{geometry}
\usepackage{vwcol} 
\usepackage{lipsum,multicol}
\usepackage[colorlinks]{hyperref} 
\usepackage{caption}
\usepackage{pgfplots}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{tikz}
\usetikzlibrary{positioning}

% Expectation %
\setlength\parindent{0pt}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}

\title{CS3243 Introduction to Artificial Intelligence}

\date{AY2019/20 Semester 2}
\author{Choong Wey Yeh}

\begin{document}

\maketitle

\section{Uninformed Search}

In this section, we will be discussing some of the basic uninformed search algorithms where these algorithms do not make use of certain heuristics to optimize their search strategy. Most of the time, we will observe that these basic uninformed search algorithms perform complete searches; that is they iterate over all the possible elements in the solution space before terminating.\\

For this section, we will be working with:
\begin{itemize}
\item \textbf{Fully-Observable Environment:} Each item in the solution space is known to the algorithm; i.e there are no hidden states
\item \textbf{Deterministic Setting:} No uncertainty/probabilistic notions involved in state change
\item \textbf{Discrete Environment:} States consist of discrete variables instead of continuous uncountable ones
\end{itemize}

Some of these examples include winning a sequence of moves in a single-player game, assembly of machine parts and boolean satisfiability(SAT) problem. Essentially, we can formulate each of these examples as a search problem where we iterate over the possible solutions. For example, in the SAT problem, we can perform a complete uninformed search over each combination of variables such that the boolean formula is satisfied. Of course, we will eventually see that this is not an efficient(nor practical) way to attain a solution for various problems.

\subsection{Problem Formulation}

Formally, we can define the basic elements of a problem definition using \textbf{states}, which are the current configurations for the problem(e.g the variable values for a SAT problem), and \textbf{actions} which causes a transition of one state to another via changing configurations(setting a boolean variable from true to false). When considering the configurations for state and actions, most of the factors are unnecessary and may be \textbf{abstracted out}. For the route planning problem in particular, we are concerned with how route the driver takes to get to a location, but \textit{not exactly how he drives there}, such as which lane or which car he uses. In this case, such information may be left out of the problem definition.\\

Additionally, we also have an \textbf{initial state}, which are the parameters that the algorithm starts with to search for the solution. This initial state can be default setting of certain parameters, or random initialization. But, we also need the following definitions:

\subsubsection{Goal State}

The \textit{goal state} is defined as the state that determines the termination of the algorithm as it has reached the objective of the solving the problem. When the search algorithm reaches a state that is deemed as the goal state, it will stop and return it's current solution. The goal state can be defined in various ways:

\begin{itemize}
\item \textbf{Explicit Set} of goal states, which can be the configurations that the algorithm permutates through. Often for problems with large solution spaces, this is not very practical as the set could be very large. 
\item \textbf{Implicit Function} $f(x)$ that evaluates the current configuration of the algorithm and outputs if the configuration is indeed a goal state or not, where $x$ is the current configuration used as the input.
\end{itemize}

Ideally, we want the evaluation of whether the current configuration is the goal state or not to be \textit{efficient}, as this evaluation function is called every single time the algorithm changes states, and could be called a large number of times(e.g if boolean formula in SAT is very long). Hence, an inefficient evaluation function could be computationally expensive and slow down the search process immensely.

\subsubsection{Path Cost}

When we are discussing about a search problem, very often each transition from one state to another incurs a certain cost or penalty, such in the route planning problem where there is a cost incurred moving from a location to another. Then, a sequence of state transitions. which are viewed as actions, is a \textit{search path} taken by the algorithm to find the optimal solution by viewing the possible actions that can be taken by the algorithm now as a decision tree like structure. We denote the cost of an action, such as a transition from one state to another as the following function:

\begin{equation*}
c(s, a, s')
\end{equation*}

where an action of $a$ causes the algorithm to transition from state $s$ to $s'$. Here, if $a_i$ is the $i^{th}$ action taken by the algorithm, then the sequence of actions $a_1, a_2, ..., a_n$ is the search path of the algorithm. If this sequence of actions results in the final state being the goal state, then it is a solution to the problem. Note that the solution \textbf{may not be unique}.

\subsubsection{Performance}

Not required for the problem formulation, but it is also eventually necessary to make sure that our algorithm works, and is efficient. In particular, we need to consider:

\begin{itemize}
\item \textbf{Termination:} We want for our solution to eventually find a solution. If the algorithm does not terminate at all, then we may never get an answer as the algorithm continues running
\item \textbf{Optimality:} We want a feasible solution, but above that we want to find the optimal feasible solution. That is, for a solution $y$, the solution is optimal if for all other feasible solutions $y'$, $c(y) \leq c(y')$. That is, the cost of our solution is minimal
\item \textbf{Efficiency:} defined as the search cost as well, is the time and space complexity of our algorithm with respect to the size of the solution space
\end{itemize}

\subsection{Search Strategies}

\subsubsection{Search Representation}

We shall represent the states and actions in our search problem as nodes and edges in a tree or graph, such as in \textbf{tree search} and \textbf{graph search}. Each node represents a state from the state space, and the edge connecting node $s$ and $s'$ represents a transition from state $s$ to $s'$.\\

Additionally, we also introduce the definition of a \textbf{frontier}. After we have reached a node $x$, the other nodes connected to $x$ are divided into those that are visited, and those that are not. The nodes that are visible from node $x$, but are not yet visited and can be expanded for exploration. These nodes form the frontier. Our search algorithm makes use of the frontier as a pool of nodes that are available to be visited next down it's search path. In tree search, we are allowed to repeat the nodes that we have visited(e.g backtracking), but in graph search we no longer return to a node once we have visited it.

\subsubsection{Search Performance}

Previously, we mentioned that the following criteria will be used to judge the performance of our algorithm:

\begin{itemize}
\item \textbf{Completeness:} Is the algorithm guaranteed to find a solution(if the solution exists)?
\item \textbf{Time Complexity:} How long does the algorithm take to find a solution in the worst case?
\item \textbf{Space Complexity:} How much space is required for the algorithm to perform the search?	
\item \textbf{Optimality:} Does the algorithm find the highest quality solution in the presence of multiple feasible solutions?
\end{itemize}

We shall explicitly evaluate the upcoming algorithms using the following parameters:

\begin{itemize}
\item \textbf{Branching Factor $b$:} The maximum number of children, or successors of a node in a search tree of graph
\item \textbf{Depth of Shallowest Goal Node $d$:} Height of the tree before the shallowest goal node is reached
\item \textbf{Maximum Depth $m$:} Maximum depth of the search tree
\end{itemize}

Instead of the standard time complexity evaluation using $V$ and $E$ as the number of nodes and edges, due to the large size of search problems.

\subsubsection{Breadth First Search}

The basic idea of BFS is that at any point of time in the search, the shallowest nodes are explored first. In particular, if the frontier consists of nodes from depth $d, d + 1,...$ then all nodes at depth $d$ are visited first before $d + 1$. BFS can easily be implemented by using a queuing function that accepts the queued neighbours of visited nodes, and outputs them in the order that it is received where newly generated states are polled last.\\

\begin{itemize}
\item \textbf{Completeness:} BFS is a very systematic strategy, where it observes all nodes of depth 1, then depth 2 and so on. Hence, at depth $d$, BFS would have finished visiting all nodes of depth $d-1$ and lesser. By the time BFS reaches depth $m + 1$, it would have visited all nodes of depth $m$ and before, which is all the nodes in the tree. Hence if a solution exists, it is sure to have found one.

\item \textbf{Time Complexity:} Consider the case where each node in the search tree has the maximum number of children $b$. Then, the first level of the search tree would generate $b$ children, and each of these children continues to generate another $b$ and so on. Then, the maximum number of nodes that the algorithm has to visit would be:
\begin{equation*}
1 + b + b^2 + ... + b^d = O(b^d)
\end{equation*}

where at the $i^{th}$ level, the search tree has $b^i$ children. Since the algorithm terminates when it has found the first goal node, if the shallowest goal node is at level $d$ then BFS would have eventually found it and does not continue past depth $d$.\\

While the time complexity is exponential in the input, BFS can be useful if heuristic says that the search tree is wide but shallow, or if the goal node is within a certain depth from the root.

\item \textbf{Space Complexity:} Similar to the time complexity, assuming the worst case scenario of a complete tree with branching factor $b$, then there are $O(b^d)$ children, and hence $O(b^d)$ amount of memory required to store all these children.

\item \textbf{Optimality:} If we consider a special case of search where the search step cost is 1, then the optimal solution for this search problem would be a feasible solution with the least number of nodes in the search path, which would be the shallowest goal node which has the lowest possible depth. Since BFS traverses the tree level by level, it would find the shallowest goal node first among all other goal nodes, which is the optimal solution.\\

However, most of the time the step cost is not unit, and in that case BFS does not usually find the optimal solution. This is because it finds the shallowest goal node, but it is not necessarily the \textit{least cost} path solution. We shall try to rectify this issue in the next search algorithm.
\end{itemize}

\subsubsection{Uniformed Cost Search}

UCS modifies the BFs strategy by always expanding the lowest cost node in the frontier, measured by a path cost function $g(n)$ as the cost of the path from the root to the current node $n$. It is trivial to see that BFS is UCS with path cost function $g(n) = depth(n)$. In implementation, we modify from BFS slightly again by changing the queue used a priority queue, or a heap data structure that emphasises cheapest paths.

\begin{itemize}
\item \textbf{Completeness:} UCS is complete, under the caveat that the minimum step cost is $\geq \epsilon$, where $\epsilon$ is a non-negative number. If there is no bound on the step cost, then we could have cases of negative costs or exponentially decreasing costs. In this case, the algorithm could be stuck without termination.

\item \textbf{Optimality:} Under the same conditions as completeness, the solution is optimal provided step costs are lower bounded by a non-negative number. This is because the first solution found by the algorithm is guaranteed to be the cheapest cost solution. If there were a cheapest cost solution, the algorithm would have found it first.\\

Note the observation that because step cost $\geq \epsilon$, the step cost must never decrease. In other words, $g(successor(n)) > g(n)$. However, if we lift the lower bound on step cost, then the search problem is reduced to that of BFS, because a long and expensive path may become the optimal by running into a node with extremely high negative cost. As a result, we need to search all possible paths to obtain the optimal.

\item \textbf{Time Complexity:} Remember that in the queue, we keep at most $b^i$ nodes in the queue if we are at depth $i$. Since the step cost $\geq \epsilon$, when the algorithm goes down a search path from depth $i$ to $i+1$, the overall current cost increases by at least $\epsilon$. Consequently, if the optimal path cost is $C^*$, then there are at most $\frac{C^*}{\epsilon} + 1$ nodes in the search path since the cost increases in sequence of $0, \epsilon, 2\epsilon,...,  \frac{C^*}{\epsilon}$. This also means the maximum depth the algorithm will go is at most $\frac{C^*}{\epsilon} + 1$, and hence the maximum number of nodes that it will visit is $b^{\frac{C^*}{\epsilon} + 1}$

\item \textbf{Space Complexity:} Similarly, since the maximum depth is $\frac{C^*}{\epsilon} + 1$, the maximum number of nodes that will be stored in the heap will be at most $b^{\frac{C^*}{\epsilon} + 1}$.
\end{itemize}

\subsubsection{Depth-First Search}

DFS expands one of the nodes the deepest level of the tree during the search instead of covering all nodes in a level. Only when the search hits the most bottom of the search path then does it go back to a shallower level and expand the nodes at those levels. This is good for search problems where it is beneficial to explore deeper rather than breadth. Implementation wise, DFS uses a stack instead of a queue to pick the next node.


\begin{itemize}
\item \textbf{Completeness:} Similar to BFS, DFS is complete as it eventually expands all nodes in a frontier and explore all possible paths, given that the depth of the search tree is \textit{finite}. For trees with infinite tree depths, then the algorithm may never terminate. 

\item \textbf{Optimality:} Like BFS, DFS is not optimal for the same reason that it does not use any heuristic on path cost at all.

\item \textbf{Time Complexity:} Assuming finite search tree height of $m$, let us consider the worst case that the optimal solution is a search path of length $m$ and that it is the rightmost path. Then, notice that because of the way DFS explores paths, DFS will exhaust all other possible paths before obtaining the optimal by depth-first traversal. If all nodes have the maximum successor branching of $b$, then that means DFS will go through all combinations of $b^m$ search paths in the worst case.

\item \textbf{Space Complexity:} DFS has a better space complexity than BFS of $O(bm)$. Each time the algorithm expands a node, it pushes all the successors of that node into the stack, which is at most $b$. Since the maximum depth, and the length of a search path is $m$, and DFS fully explores and exhaust a search path before it expands another node, the maximum number of items pushed on the stack is $bm$. \\

In fact we can continue to improve this space complexity further. Notice when we push the successors onto the stack, we pick one successor to continue exploring down. In this process, other successors are not used until all the possible paths following the picked successor is exhausted. In that case, we merely have to push the \textit{current} successor onto the stack to be tracked without the other successors. The algorithm can then backtrack and then pick another successor to expand along later in the search. Then, the space complexity is equivalent to the number of successors picked in a path, which is also equivalent to the maximum path length that is $O(m)$.
\end{itemize}

\subsubsection{Depth-Limited Search \& Iterative Deepening Search}

We have seen that for trees with infinitely long tree heights, then the algorithm could potentially run forever. If we knew that we only had limited time to run our search, or that the goal node is within a certain height, then we could limit the maximum height that DFS runs to. That is, we can force our algorithm to terminate once there are no longer any more search paths with length $l \leq l*$ where $l*$ is the maximum height specified.\\

However, note that if we do so, then our DLS algorithm is \textbf{not complete}. Because it only considers search paths of length $l^*$ and lesser, any search paths with longer lengths is ignored. If the goal node is of length $\geq l*$, then our algorithm would never return a solution.\\

To address this, we introduce an additional condition: To perform DLS \textit{until a solution is found}. This guarantees that the algorithm is complete and will return a solution to us. Additionally, this algorithm is similar to BFS in the sense that the search deepens level by level, like how BFS expands nodes in a depth first.

\begin{algorithm}
\caption{Iterative Deepening Search}\label{euclid}
\begin{algorithmic}[1]
\For{$depth = 0 \rightarrow \infty$}
	\State \textit{result} $\leftarrow$ DEPTH-LIMITED-SEARCH(\textit{problem, depth})
	\If{$result \neq $ cutoff}
		\State \textbf{return} \textit{result}
	\EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

This allows us to maintain the space complexity benefit of DFS over BFS, but also allows us to address the problem of large state space and unknown tree depth(which is often most of the time).\\

However, now we have another downside: we essentially repeat searches over subtrees of the search tree. We first perform DLS with depth $d$, then $d + 1$ and so on. Notice that the search space on the search tree limited with depth $d$, is a subspace of the search space on a tree with depth $d + k$. Hence, we are repeating some of the search from the previous iteration of IDS. In particular, the number of nodes expanded by DLS on depth $d$ is:

\begin{equation*}
1 + b + b^2 + ... + b^{d-1} + b^d
\end{equation*}

and the number of nodes expanded by IDS is:

\begin{equation*}
(d+1)1 + (d)b + (d-1)b^2 + ... + 2b^{d-1} + 1b^d
\end{equation*}

Notice that of all the subtrees of different depths of the search tree, IDS explores subtrees of shallowest heights the most, while the bottom-most parts of the trees is explored lesser. This is because of the fact that in every iteration of IDS, it performs DLS restarting from the root again. Hence the initial parts of the search tree are traversed more often. \\

In particular, the further down the tree the search goes, the number of successors increases, perhaps exponentially. Since these parts of the tree are explored much lesser than the top, which has much lesser nodes, in perspective the search is repeated on a very small subset of the nodes of the entire search space considered, and hence the overhead in practice is actually within accepted bounds.

\paragraph{Remarks}
\begin{itemize}
\item If $b$ is not finite, then all the uninformed algorithms we have discussed are not complete, since they will never be able to exhaust fully nodes of any depth
\item If $b$ is finite, then BFS and IDS is complete, since IDS iteratively increases the depth similar to BFS. Additionally, if the step cost is lower bounded by $\epsilon$, then UCS is also complete
\item If $m$ is finite, then DFS is complete, otherwise it may continually run forever down an infinite search path
\item if the step cost is unit(i.e = 1), then UCS is reduced to BFS, and BFS and IDS are optimal because the step cost is equivalent to the number of nodes in search path
\end{itemize}
\pagebreak

\section{Informed Search}

Previously, we have seen search strategies that makes use of the problem definition, such as the states and actions, to perform a general search action. This usually results in some form of queuing function that determines which node to expand next deterministically. However, more often than not we do not have enough information to full model the problem to determine the full cost. For example,

\begin{itemize}
\item Depth of search tree or size of solution space could be possibly infinite
\item Repeated states could be present
\item Certain constraints could be hard to model, or cause problems to be difficult to solve
\end{itemize}

If we use uninformed search strategies for these problems, optimality and completeness may not be guaranteed, in addition to the exponential time search efficiency. In this case, we can introduce additional information or heuristics that can be exploited in order to improve our search strategy. To do this, we introduce an evaluation function $f(n): X \rightarrow R$ which returns a number indicating the desirability of the node to expand next. This could be in terms of shortest path cost for example. The evaluation function that we define here serves to approximate the cost estimate from node to node in problems where full information is not present for us to perform uninformed search.

\subsection{Best-First Search}

In best-first search, we make use of the evaluation function by expanding the node that has the most desirable evaluation function score, by ordering the nodes by their evaluation function values. If the evaluation function is able to perfectly reflect the information stated in the problem definition, that is if the evaluation function is a true one, then expanding the node by the evaluation function score will be the optimal path and indeed be the best node to expand at the current point in time. However, this is often not the case and the evaluation function will reflect the node that \textit{appears} to be the best node to expand in that point in time.\\

\begin{algorithm}
\caption{Best-First Search}\label{euclid}
\begin{algorithmic}[1]
\State $result \leftarrow$ next node ordered by $f(n)$
\If{$result == goal\ state$}
	\State return $search\ path$
\Else
	\State goto $1$
\EndIf
\end{algorithmic}
\end{algorithm}

\subsubsection{Greedy Best-First Search}

A good Best-First Search strategy is to \textit{minimize estimated cost to reach the goal}. That is, we define a \textbf{heuristic function} $h(n)$ that estimates the cost from a node to the goal node. Often this function provides not an exact formulation of the actual cost from a node to the goal node, but a good enough estimate of this cost. Then, we set:

\begin{itemize}
\item $f(n) = h(n)$ and use the heuristic function to expand the next node
\item $h(t) = 0$ where $t$ is the goal node, as the distance from the goal node to itself is clearly 0
\end{itemize}

\subsubsection{Performance Measure}

\begin{itemize}
\item \textbf{Completeness:} Similar to the uninformed search functions, Best-First Search also makes use of a queuing function, albeit using an evaluation function. This means that every node will eventually be added into the queue, and expanded until the solution is found if $b$ is finite. 

\item \textbf{Optimality:} However, it is easy to see that Best-First Search \textbf{is not optimal}. Notice that Best-First Search aims to obtain low cost solutions by using the estimated cost from the node in the frontier to the goal node. However, \textit{it does not make use of past information of the cost of the path travelled so far}. Best-First Search focuses on greedily reducing the current estimated remaining cost to the goal, and completely ignores the overall path cost to reach the goal node.\\

In order to focus the search on lowest cost paths toward the goal, our evaluation function must incorporate the \textit{cost of the path} from a state to the goal state, instead of ignoring the path of the cost so far to the current node.

\item \textbf{Time Complexity:} In the worst case where the path containing the least cost weights has length of that of the height of the search tree, then the search will incur runtime exponential in the order of the branching factor $O(b^m)$

\item \textbf{Space Complexity:} Similar to the time complexity, in the worst case we may maintain $b^m$ elements in the queue for a search tree with branching factor of $b$ and a search tree depth of $m$.
\end{itemize}

Apart from in-optimality issues due to ignoring of overall path cost, Best-First Search also has various other issues:

\begin{itemize}
\item \textbf{Susceptible to False Starts:} Because the search strategy only minimizes the current remaining estimated cost, it could be possible for the algorithm to pick the shortest node that leads to a dead end, resulting in unnecessary nodes being expanded.
\item \textbf{Repeated States:} Furthermore, if we allow repeated states such as in Graph Search, then it could be possible for the algorithm to oscillate between 2 visited states if the estimated cost between the two is the cheapest among those in the frontier.
\item \textbf{Infinite Search Path Length:} Similar to Depth-First Search, if the search tree path has infinite length, then it is possible for the algorithm to keep searching down that path and never terminate
\end{itemize}

\subsection{A* Search}

We have seen that minimizes the estimated cost to the goal using the heuristic function $h(n)$ in a bout to reduce search cost, but it is not optimal. On the other hand, Uniformed-Cost Search(UCS) reduces the cost of the path so far to the goal, but it is inefficient due to it's exponential time complexity. But we can combine both strategies to obtain a search strategy with the advantages of both. We do this by summing up the the evaluation functions of both search algorithms where:

\begin{itemize}
\item $g(n)$ is the cost function from UCS that measures the cost from the start node to the current node $n$
\item $h(n)$ is the cost estimate heuristic from Best-First Search that gives an estimate of the remaining cost from the node $n$ to the goal node
\item $f(n) = g(n) + h(n)$ is the combined heuristic function that estimates the cost of the cheapest path from the start node to the goal node by expanding node $n$ from the current node.
\end{itemize}

Hence if we are aiming to minimize the overall cost of the search path, then minimizing $f(n)$ is a reasonable strategy. In fact, we will soon prove that given the following restriction on $h(n)$, then the search strategy is complete and optimal:

\begin{theorem}
If $h(n)$ is an \textbf{admissible heuristic}, then A* Search using Tree Search is optimal
\end{theorem}

\begin{definition}
A heuristic function $h(n)$ is an \textbf{admissible heuristic}, if $\forall n, h(n) \leq h^*(n)$ where $h^*(n)$ is the optimal true cost to reach the goal state from node $n$. 
\end{definition}

That is, the algorithm \textit{never overestimates the true cost of the path from node $n$ to the goal}.\\

Furthermore, we shall assume the \textbf{monotonic} property for the path costs for A* search that, all path costs are non-decreasing. That is for the path containing the node $n$ to the goal node $t*$, $f(n) \leq f(t*)$. In particular we say that the heuristic function is \textbf{consistent}.

\begin{definition}
A heuristic function $h(n)$ is \textbf{consistent}, if $\forall n$, and it's successors $n'$, $h(n) \leq c(n, n') + h(n')$.
\end{definition}

In some cases, we also call this heuristic function metric because it obeys the triangle inequality. A result from this definition is that path costs are non-decreasing, or monotonic. This can be seen via the following equation:

\begin{equation*}
\begin{aligned}
f(n') & = g(n') + h(n') \\
& = g(n) + c(n, n') + h(n') \\
& \geq g(n) + h(n) = f(n)
\end{aligned}
\end{equation*}

In particular, the following lemma holds:

\begin{lemma}
If a heuristic is consistent, then it is also admissible
\end{lemma}

A simple proof of this is that, we can take any node $n$, then we see that:

\begin{equation*}
\begin{aligned}
h(n) & \leq c(n, n') + h(n')\\
& \leq c(n, n') + c(n', n'') + h(n'') \\
& ...\\
& \leq c(n, n') + c(n', n'') + ... + c(n^{t-1}, t) + h(t) \\
& \leq c(n, n') + c(n', n'') + ... + c(n^{t-1}, t) (\text{remember $h(t) = 0$})\\
& \leq c(n, t)
\end{aligned}
\end{equation*}

If the path costs for a problem is non-monotonic, then we can use the following function to enforce monotonicity using the following $pathmax$ function:

\begin{equation*}
f(n') = max(f(n), g(n') + h(n'))
\end{equation*}

By the monotonicity property of the path costs, that means that A* will expand all nodes $n$ with evaluation function costs $f(n) < f*$ where $f*$ is the cost of the optimal solution path. This is similar to the BFS behaviour where A* expands nodes layer by layer, where each layer is a contour represented by it's evaluation function cost.

\subsubsection{Proof of Optimality of A* Search}

Let the optimal goal state in the search tree be $t*$, and a suboptimal goal state be $t$. Then, $f(t) = g(t) \leq f(t*) = g(t*)$. Let us then assume then A* algorithm has picked $t$ as the solution and terminated. We shall see that this is not possible.\\

Let $n$ be an unexpanded node on the path to the optimal goal state $t*$. Such a node exists because if it does not, that means that the algorithm that already expanded all nodes on the path to $t*$, and hence found the optimal goal state. Additionally, let it be such that 

\begin{equation*}
f(t) \leq f(n)
\end{equation*}

such that $t$ is expanded before $n$, which can be rewritten as:

 \begin{equation*}
 \begin{aligned}
f(t) \leq f(n) & = g(t) + h(t) \leq g(n) + h(n)\\
&= g(t) \leq g(n) + h(n)
\end{aligned}
\end{equation*}

since $h(t) = 0$ as $t$ is a goal state. Then, because path costs are non-decreasing, that means that:

 \begin{equation*}
 \begin{aligned}
g(t) & \leq g(n) + h(n) \\
& = f(n) \\
& \leq f(t*) \\
& = g(t*) + h(t*) = g(t*) (\text{since $h(t*) = 0$})
\end{aligned}
\end{equation*}

Which means that the path cost of reaching the suboptimal goal state $t$ is lesser than that of the optimal goal state $t*$, which contradicts our initial assumption that $f(t) = g(t) \leq f(t*) = g(t*)$. Hence, we have proven that A* search is guaranteed to find the optimal goal state. Intuitively, we can see that this is true because $f(t) \leq f(t*) = g(t*) = g(n) + h*(n)  \leq f(n) = g(n) + h(n)$ for any node $n$ that is on the optimal path from the start node to the goal node due to non-decreasing costs, and by the admissible heuristic that $h(n) \leq h*(n)$. Moreover, it is also easy to see that if we have a heuristic that \textit{overestimates} the path cost to the goal node, then the algorithm might evaluate $f(n) > f(t)$ and eventually find the suboptimal goal first.\\

Additionally,  a stronger claim is that when a node $n$ is selected for expanding, that means that \textit{the path leading up to $n$ is the shortest path found}. To see why this is so, we shall assume that the algorithm expands node $n$ via a suboptimal path with last node $q_2$ instead of a optimal path with last node $q_1$. We know that since $n$ was expanded before $q_1$, that means that $f(q_2) \leq f(n) \leq f(q_1)$. But this contradicts the optimality of the optimal path with cost $f(q_1) < f(q_2)$, and the non-decreasing property since $q_1$ is on the path to $n$ and hence $f(q_1) < f(n)$.\\

In particular, it is also proven that A* is \textbf{optimally efficient}. That is, any other search algorithm that uses an admissible heuristic that finds the optimal goal state \textit{does not expand less nodes} than A*. This also means any algorithm that does not expand nodes $n$ with $f(n) 
\leq f(t*)$ run the risk of non-completeness and not finding the optimal goal state.

\subsubsection{Proof of Completeness of A* Search}

Recall that we mentioned that path costs are non-decreasing along the path. We can visualize the search landscape of the search tree in contours, where each contour is in order of increasing path costs. We can see that A* Search eventually expands all nodes in a contour before moving onto the next. In particularly, A* search moves in increasing order of contours using the evaluation function. This means that A* search will expand up the contours, before it eventually reaches the optimal shallowest goal node.\\

However, this is only true under 2 conditions:
\begin{itemize}
\item If there are finite number of nodes with evaluation function costs $f(n) < f*$
\item If the branching factor $b$ in the search tree is finite
\end{itemize}

Similar to problems with uninformed search, we can see that if the above 2 conditions do not hold, it is possible for the algorithm to be stuck within a contour and never terminate.

\subsubsection{Time and Space Complexity of A* Search}

While A* Search is optimally efficient, it still incurs time complexity exponential in the base $b$, to the power of the error incurred in the estimation of the heuristic function resulting in $O(b^{h^*(n) - h(n)})$, unless the error grows no faster than the logarithm of the actual path cost(proof omitted):

\begin{equation*}
|h^*(n) - h(n)| \leq O(\log h^*(n))
\end{equation*}

In particular, we wish to use dominant heuristic functions over a set of possible $h(n)$

\begin{definition}
For 2 heuristic functions $h_1(n), h_2(n)$ that are admissible, if $\forall n, h_1(n) \geq h_2(n)$, then $h_1(n)\ \textbf{dominates}\ h_2(n)$
\end{definition}

This also means that $h_1(n)$ incurs lower search costs, because since both functions are admissible, values for $h_1(n)$ are closer to the true cost function $h^*(n)$ than $h_2(n)$ is.\\

This means that A* is greatly affected by the error incurred by the heuristic function estimation. However, computational time is not the biggest drawback. A* incurs a space complexity of $O(b^m)$ by maintaining nodes generated in the frontier in memory. More often than not, A* Search will run out of space before it finishes running.

\subsection{Deriving Admissible Heuristics}

Often we have to invent our own heuristic functions in order to tackle a search problem. One of the ways that we can do this is by \textbf{relaxing} the constraints to the problem. This means removing some of the restrictions placed on the problem itself, leading to special cases of the problem that can be solved using specific derivable heuristics. It is often that \textit{optimal solutions to a relaxed problem is often a good heuristic to solving the original}. By relaxing a problem, we can develop a set of admissible heuristic functions $\{h_1, h_2,..., h_n\}$ that provides optimal or near optimal solutions to specific configurations to the search problem, which can be used to evaluate different scenarios.\\

With a set of specific heuristic functions, it follows that each of these heuristics that developed to solve a specific scenario, and not a general best heuristic for the best solution itself. If we pick one heuristic over another, it could be that the heuristic does optimally in one scenario, but very bad in another. Hence instead of picking, we can make use of the dominant property to pick heuristic functions at the current state:

\begin{equation*}
h(n) = \max (h_1, h_2, ..., h_n)
\end{equation*}

Which means that always have the best option of a heuristic at any state in the search. Using the dominant property, as well as the fact that the heuristics are admissible, guarantees the best choice of heuristics among all we have by picking the heuristic closest to the upper bound of the true cost.\\

Other methods of derivation of heuristics involve statistical learning and feature evaluation as well, which allows the algorithm to derive importance of various features of heuristics in different situations by itself.

\pagebreak
\section{Local Search}

In some problems, the state description contains all the information about the problem that we need to make informed searches, and the path in which we take during our search may not be so useful anymore. We only require that the goal state be reached. In this case, local search, or \textbf{iterative improvement} algorithms may provide a practical approach that does not require searching over search paths. More often than not, it is often a good idea to \textit{start with a complete configuration of the problem, and make gradual improvements to the solution}. We can easily see that such algorithms search a smaller portion of the search space while still being able to provide reasonable solutions to the problem. Local search algorithms do not look far into a search path, and usually only considers neighbours within close proximity.

\subsection{Hill-Climbing Search}

The hill-climbing algorithm is one that continually moves the current position in the direction of an increasing objective value. The algorithm always aims to make an improvement through a movement to it's surrounding neighbours if possible. Variants of this algorithm in other problems such as convex optimization may also be known as  gradient descent.\\

\begin{algorithm}
\caption{Hill-Climbing Search}\label{euclid}
\begin{algorithmic}[1]
\State $current \leftarrow problem.$initial-state
\Loop
	\State $neighbour \leftarrow$ highest-valued successor of $current$
	\If{neighbour.Value $\leq$ current.Value}
		\State \Return current.State
	\Else 
		\State $current \leftarrow neighbour$
	\EndIf
\EndLoop
\end{algorithmic}
\end{algorithm}

Note that unlike the previous algorithms that we have seen, the Hill-Climbing search algorithm does not maintain any form of search tree or graph, but only the current state, it's neighbour states and their evaluations. This cuts down the need for large memory requirements immediately.\\

However, the Hill-Climbing search paradigm does suffer from certain drawbacks:

\begin{itemize}
\item \textbf{Local Maxima:} The algorithm may run into a point where the current state is a peak among all other neighbours, and returns this state. That is, the algorithm is guaranteed to return a \textit{maxima}, but it could be a local maxima and be maximal with respect to the neighbours, but not the entire state space and be a \textit{global maxima}. The algorithm does not know this and only recognises the local maxima as the highest value, since it does not look past the neighbours of the current state. In many cases, the local maxima returned by the algorithm may be far from satisfactory compared to the global maxima.

\item \textbf{Plateau:} This is an area of the state space where the value does not change from current to neighbouring states. In this case, the algorithm does not know for sure which move to take, and either uses heuristics to decide or makes a random walk.
\end{itemize}

The drawbacks that Hill-Climbing search has points to the fact that once the algorithm is unable to make any further improvement, it returns the current best solution without checking other peaks in the state space, and most of the time the current highest returned is a local maxima and not the optimal solution to the problem. That is, Hill-Climbing fully uses exploitation, but not exploration in it's search. We can combat this in various ways depending on the problem, but the most commonly known is \textbf{Random-Restart Hill-Climbing} by restarting the algorithm initialized to different starting states in a bout to obtain a higher maxima.

\section{Adversarial Search}

Unlike previous search problems that were defined before, we now look at problems where \textit{more than one player} is involved. For example, chess can be modelled as a search problem playing against another player and trying to make the best moves at each point in time. These problems are also called \textit{games}, and model real world problems much more accurately than previous problems.\\

Unfortunately, the presence of additional agents also introduce multiple factors that increase the intractability of solving the problem:

\begin{itemize}
\item \textbf{Uncertainty} of the next environment states in the game, since the states are no longer influenced just by the agent's actions, but also other agent's actions as well.

\item \textbf{Larger search space} due to the increase in the total number of combinations of moves resulting from additional agents participating in the game. The agent needs to account for the possible moves that the opponent might take in chess, for example.
\end{itemize}

\subsection{Two-Person Games}

Let us being by considering games that involves 2 agents or players. In particular, we wish to look at \textbf{zero-sum games}, which are games where the gain or loss by a participant is exactly balanced and reflected in the other player's utility. For example, in a gambling problem involving a banker and the player, if the banker has a utility of $x$, that means that he/she has won $x$ dollars and consequently the player has lost $x$ dollars and has a utility of $-x$. Additionally, we reformulate the following:

\begin{itemize}
\item \textbf{Initial state} includes the starting board position, and who goes first
\item \textbf{Set of operators} determines the legal moves that the current play can make
\item \textbf{Terminal test} defines whether or not the game is over depending on win conditions, or if no more legal moves can be made
\item \textbf{Utility/Payoff Function} returns a numerical value on the outcome of the game. In the game of chess, a win for the MAX player would give +1, a win for MIN player would give -1 and a draw would be 0
\end{itemize}

We also see that for zero-sum games, we define a MAX and MIN player, that aims to maximise and minimize the utility function respectively. We can see that the 2 agents now have objectives that contradicts each other, and makes the game harder for the opponent. More importantly, any gain or loss for the MAX player is a loss or gain for the MIN player respectively, and we can see that the total utility \textit{sums to 0}, which reflects the definition of a zero sum game.\\

\subsubsection{Minimax Algorithm}

Let us consider the search tree for formulating a 2 player zero-sum game. Note that the plays alternate between the MAX and the MIN player, and hence each depth of the search tree alternates between player MAX and MIN. The minimax algorithm uses this search tree to formulate the optimal moves for the MAX player that starts the game first, by deciding the best optimal starting move.

\begin{enumerate}
\item The algorithm first generates the entire search tree for the game, all the way to the terminate states that are represented by the leaves in the search tree. Each depth of the tree alternates between MAX and MIN's moves, with the root of the tree representing the starting move by MAX

\item From the terminal states at the leaves, we apply the utility function to each leaf to determine the the utility value. This represents a +1 for a win for MAX, and -1 for a win by MIN for a game of chess for example.

\item We recursively move one level up the tree everytime the utility values of the current depth is determined for all nodes, and use the utility value for the current depth to decide the utility value for the nodes one level up. In particular, let $S_i$ be the set of children residing in depth $i$ of node $v_{i+1}$, and $f$ the utility function used in the game. If level $i+1$ denotes a play by MAX, then 

\begin{equation*}
f(v_{i+1}) = \max_{u \in S_i} f(u) \text{ if player = MAX}
\end{equation*}

That is, at level $i+1$, MAX will pick the move out of all possible children that will lead it to the terminal state that yields the highest possible utility. Similarly for MIN, if the current level corresponds to a play by MIN, then 

\begin{equation*}
f(v_{i+1}) = \min_{u \in S_i} f(u) \text{ if player = MIN}
\end{equation*}

instead to minimize the overall utility.\\

In particular, we see that everytime the levels alternate, the utility switches from maximizing to minimizing based on the MAX and MIN player. This is because the 2 objectives of the players are clashing, and everytime MAX makes a move that maximizes the utility, in the next half-move by MIN player, he/she will counter MAX's progress by choosing a path that minimizes the overall utility. As such, MAX and MIN never really often attain the maximal/minimal utility, but rather, the \textit{maximum minimal utility} for MAX, and vice versa for MIN. Hence the name, Minimax algorithm.
\end{enumerate}

In particular, at each level of the search tree, the move taken by the player to maximize/minimize the utility is known as a \textbf{strategy}.  

\begin{definition}
A strategy $s_1$ for player 1 is called \textbf{winning} if for any strategy $s_2$ by player 2, the game ends with player 1 as the winner.\\
A strategy $s_1$ for player 1 is called \textbf{non-losing} if for any strategy $s_2$ by player 2, the game ends in either a tie or a win for player 1.
\end{definition}

Another important observation is that at each level, the player that makes the play at that depth plays the best that they can to possibly maximize/minimize the utility, and we cannot change the search path choice to improve the utility. The minimax algorithm is also known to output a \textbf{sub-perfect Nash equilibrium}. While the choice over the entire tree may not be optimal, but at each play, the player makes the best choice they possibly can.

\begin{itemize}
\item \textbf{Completeness:} If the maximum depth of the tree $m$ is finite, and the number of legal moves(which translates to the number of branches $b$) is finite, then the algorithm is able to fully generate the tree and is complete.

\item \textbf{Optimality:} Similar to completeness, since $m$ and $b$ are finite, the entire search tree with it's respective utility costs is visible to the algorithm. That means that the algorithm has perfect information to the utility cost of each move, and is able to make the best move that maximizes/minimizes it's utility. Hence, it would be able to output the optimal solution.

\item \textbf{Time Complexity:} For a search tree of branching factor $b$ and depth $m$, in the worst case it needs to generate a tree with $b^m$ nodes, and hence takes $O(b^m)$ time to do so

\item \textbf{Space Complexity:} Because of the recursive nature of generating the nodes, Minimax algorithm maintains nodes in a search path by diving into the terminal states before going back up, using at most $O(bm)$ space.
\end{itemize}

\subsection{Alpha-Beta Pruning}

We have seen that for the minimax algorithm to output the optimal solution, it needs to generate the entire search tree which hinges on it being finite. However, even for finite trees, it can be seen that sometimes search trees can be very large, and it may take a long time for the algorithm to perform search.\\

Fortunately, it is possible for Minimax to be optimized to search and output the optimal solution despite not looking at every single node in the search tree, using the intuition that \textit{we do not explore nodes that are already bad}. Consider a node $n$ along the search tree. If the player has a better choice at node $m$ which is a parent of, or higher up the search tree from $n$ that improves his/her utility, then node $n$ will never be reached. Hence, it is not necessary to explore the paths further down from $n$.\\

Since Minimax is depth first, we only need to consider nodes along a single path in the tree. We denote $\alpha(n)$ as the highest observed value found on any path from $n$ initialized at $- \infty$, and $\beta(n)$ as the lowest observed value initialized at $\infty$. Intuitively, this can be seen as $\alpha$ is the maximal possible utility for MAX, and $\beta$ the minimal for MIN, and any further values of $\beta$ that do not improve the value of the current $\alpha$ will be pruned. In particular,

\begin{itemize}
\item given a node $n$ at a level corresponding to a MIN play, we stop searching below $n$ if there is a MAX ancestor $i$ of $n$ such that $\alpha(i) > \beta(n)$
\item given a node $n$ at a level corresponding to a MAX play, we stop searching below $n$ if there is a MIN ancestor $i$ of $n$ such that $\beta(i) < \alpha(n)$
\end{itemize} 

Note that alpha-beta pruning prunes aways branches of the search tree does will never get reached, and hence these branches do not influence the final decision. But how much computational effort does alpha-beta pruning actually save? This depends on the ordering of the moves that are legal, which determines the search path ordering. With perfect ordering, alpha-beta pruning can give a time complexity on Minimax of $O(b^\frac{m}{2})$. This means that the algorithm can actually search twice as deep with the same time constraints. However, even without a good ordering, on average alpha-beta pruning gives a time complexity of $O(b^\frac{3}{4})$ in a random ordering.

\subsection{Evaluation Functions}

When search gets large, we also have the option of limiting search, just as in DLS. However, for cases like Minimax algorithm, we run into several major problems, the first being that since depth is limited, we may not reach the terminal states for certain search paths. Because the utility of intermediate noes rely on the terminal states to determine the actual utility, that means that we are unable to determine intermediate nodes of search paths that do not reach the terminal state.\\

We rectify this by introducing an \textbf{evaluation function}, which is an approximation function that evaluate how good an intermediate state is. We require that the evaluation function satisfy the following conditions:

\begin{itemize}
\item The evaluation function must agree with the utility function on terminal states
\item The evaluation function cannot be too expensive, in particular it cannot be more expensive than the utility function as an approximation. 
\item The evaluation function must be accurate in reflecting the player's odds of winning
\end{itemize}

Additionally, we also need to ensure that we do not run into the \textbf{horizon problem} when we perform the search limited. If we cut off search at a state that is volatile, and is threatened by possible wild swings or moves that may turn the game around, then it could be possible that the winning odds reflected at the state before and after the cutoff reflect very different information. Hence, we also include the condition that the search be performed til a \textbf{quiescent} position before being cut off, which is one that is robust to large changes.

\subsection{Expectimax Algorithm}

In some games that involve stochastic layers, we also need to incorporate randomness into the game itself such as Poker. In addition to our standard minimax algorithm search tree, we also need to insert 'chance' nodes that represent as random choice between state changes. For each of the children of chance node $i$, $x_j$, we represent the probability that the search goes down to node $x_j$ is denoted $p(x_j)$. At the terminal nodes, we continue to use our utility function to obtain the utility value. However when we propagate upwards to the chance nodes, then we are unsure of exactly which path we will take. Instead, we take the $expected$ utility value upon reaching that node, which is the combination of the utility values among all children weighted by the probabilities of hitting that node.

\begin{equation*}
\begin{aligned}
f(x_i) = E[f(x_i)]  = \sum_{j \in J} p(x_j) f(x_j)
\end{aligned}
\end{equation*}

By introducing chance nodes and randomness into our search algorithm, we have increases the time complexity from $O(b^m)$ to that of $O(b^mn^m)$ because of the fact that we could have up to $n$ chance nodes at each of the $m$ levels.

\pagebreak
\section{Constraint Satisfaction Problems}

In some problems, in addition to the basic requirements of obtaining the optimal solution or reaching the goal state, we also need to satisfy additional requirements or properties for the solution to be feasible. These properties are called \textbf{constraints}, and the problem can be modelled as what is called a \textbf{Constraint Satisfaction Problem(CSP)}. In CSPs, the states are represented in the form of variables, and constraints that the goal state must obey. For example:

\begin{equation*}
\begin{aligned}
&\min c^T x\\
&s.t \textbf{	} Ax = b\\
&x \geq 0
\end{aligned}
\end{equation*}

is commonly seen as the standard form of the Linear Programming problem. \\

Each variable $x_i$  has a domain $D_i$ that it takes it's values from, such as $D_i = \mathbb{R}$ for real numbers, or $D_i = \{0, 1\}$ for binary values.This domain can be discrete or continuous, but CSP under continuous domains falls under the previously mentioned Linear Programming problem, and for this section we focus on discrete values. Additionally, constraints can be the following:

\begin{itemize}
\item \textbf{Unary Constraints} involving one variable: $x_i = a$
\item \textbf{Binary Constraints} involving 2 variables: $x_i + x_j \leq a$
\item \textbf{Higher Order Constraints} involving multiple variables
\end{itemize}

Note that in this case, we are using integer operations on constraints. This is defined by our \textbf{Constraint Language}, which represents \textit{how} our constraints should be formulated.\\

Converting our CSPs back to the form of search state spaces, we can denote our initial state as an empty assignment; that is no values are assigned to any variables, and the transition functions being the assigning of values to variables. Lastly, our goal state is an assignment for the variables such that the solution is feasible. In particular, we say that the assignment is $complete$ if the assignment does not violate any constraints in the CSP, and $\forall i, y_i \in D_i$ .\\

A standard search paradigm for working with CSPs is as follows:

\begin{algorithm}
\caption{Standard CSP Search}\label{euclid}
\begin{algorithmic}[1]
\State $assignment \leftarrow []$
\While{$assignment$ is not complete}
\State $assignment \leftarrow assignment \cup {y_i}$ where $x_i = y_i, y_i \in D_i$
\State \If{$assignment$ is complete} 
	\Return $assignment$
\EndIf
\EndWhile
\end{algorithmic}
\end{algorithm}

Note that there are a few quirks of simply following the above routine:
\begin{itemize}
\item \textbf{Repeated States} but different order of assignments. This means that in term of a search tree, then we would have many unnecessary nodes that represent the same state(e.g assigned $x_i = a$ then $y_i = b$ in one path and the reverse order in another path. Notice order is not required. We can merely represent each level of the search tree as the assignments for a $single variable$. Then, for $d$ dimensional variable and $m$ variables, the search tree is at most $O(d^m)$. This is also known as \textbf{Backtracking Search}, where we assign a variable per level.

\item \textbf{Running into already invalid assignment} but still continuing to search. In many cases, we can already determine if progress from the current state will lead to a valid assignment. If the current state can no longer reach a valid assignment, we can simply stop the search down that sub tree and effectively prune search in that subtree.
\end{itemize}

\subsection{Backtracking Search}
\begin{algorithm}
\caption{Backtracking Search}\label{euclid}
\begin{algorithmic}[1]
\State $assignment \leftarrow []$
\While{$assignment$ is not complete}
\State \If{$assignment$ is complete} 
	\Return $assignment$
\EndIf

\State $var \leftarrow$ Select-Unassigned-Variable$(csp)$
\For{each value in Order-Domain-Values$(var, assignment, csp)$}
\If{$y_i$ is consistent with $assignment$}
\State add $\{ x_i = y_i\}$ to $assignment$
\State $inferences \leftarrow$ Inference$(csp, var, value)$
\If{$inferences \neq failure$}
\State add $inferences$ to $assignment$
\State $results \leftarrow$ Backtracking Search($assignment, csp$)
\If{$result \neq failure$}
\Return $result$
\EndIf
\EndIf
\EndIf
\EndFor
\EndWhile
\end{algorithmic}
\end{algorithm}

Notice that 
\begin{itemize}
\item \textbf{Order of variable assignment is irrelevant}, similar to how we do not bother about the search path and concentrate on finding a valid assignment
\item \textbf{At every level, consider a single variable}. This considerably reduces our search space for assignments
\item \textbf{Use of inferences}, which tells us what we can say about the remaining values given the current assigned values. That is, the currently assigned values determine which of the remaining values are valid or not.
\end{itemize}

In particular, we can improve efficiency and save runtime by optimizing the subroutines: \textbf{Select-Unassigned Variables, Order-Domain-Values and Inferences}. Basically, \textit{how do we choose what to assign each variable at each point in time}. The following as heuristics we can use:

\begin{itemize}
\item \textbf{Minimum Remaining Values(MRV):} Pick the variable with the \textit{fewest} legal values left. This allows us to detect  invalid assignments quicker before searching too deep and wasting search time.
\item \textbf{Degree(Most Constraining Variable) Heuristic:} Pick the variable that has the most relations with other variables. That is, it \textit{influences} other variables the most. Similar to MRV, selecting this variable first allows us to quick eliminate assignments to other variables made invalid because of this variable quickly.
\item \textbf{Least Constraining Value} for \underline{value selection} to be assigned to the variable. We want values assigned to variables to be permissive, so that we are more likely to find a valid assignment and not waste time searching only to reach an invalid solution.
\end{itemize}

\subsection{Inferences}

\subsubsection{Forward Checking}

In forward checking, the main idea is to keep track of remaining illegal values for unassigned variables. At any point in the time in the search, if any of the variables has no remaining legal values left, then it ay progress cannot output a valid assignment since one variable cannot be satisfied. Hence, we terminate search down the current subtree and return back to a state where a valid assignment is still possible.

\subsubsection{Constraint Propagation}

Using forward checking, we are able to terminate search down a subtree that is guaranteed to not have a valid assignment, and hence prunes portions of the search tree to reduce search space. But we can improve this further. Note that forward checking does not detect early signs of invalid assignments. This is where \textbf{constraint propagation} comes in. Using constraint propagation, we are able to observe if values are illegal by reducing and narrowing down the search space by removing assignments that have illegal values. In particular, constraint propagation enforces constraints \textbf{locally} for certain variables in order to continue search.

\subsubsection{Arc-Consistency}

\begin{definition}
$X_i$ is \textbf{arc-consistent} with $X_j$ iff for every value $x_i \in D_i$, there exists some value $y_j \in D_j$ such that the binary constraint on the arc $(X_i, X_j)$ is satisfied
\end{definition}

That is, an arc $(X_i, X_j)$ is arc-consistent if there is a valid assignment for $X_j$ for every value that we assign to $X_i$. As we perform search down the tree, it is likely that most of the variables in the state that we are in or will be in, will not be arc-consistent with every other variable. If that is the case, then most likely whatever value we pick for the variable $X_i$, the leading assignment will violate some constraint since it is not arc-consistent. However, we can \textbf{enforce} arc-consistency by removing illegal values from the domain $D_i$ such that the resulting domain $D_{i}^{'}$ is arc-consistent with every other variable. Then, we have effectively reduced the domain $D_i$ for $X_i$ to one that contains legal values that do not lead to illegal assignments. This is done through:

\begin{itemize}
\item \textbf{Enforcing unary constraints} on the variable $X_i$. That is, we prune values from $D_i$ that do not enforce unary constraints $X_i \geq a$ for example.
\item \textbf{Propagating arc-consistency on binary constraints}. By enforcing arc-consistency on the arc $(X_i, X_j)$, then the arc-consistency on the neighbouring arc $(X_k, X_i)$  can be enforced on $D_k$ on the reduced domain $D_{i}^{'}$. This could not have been possible without enforcing arc-consistency on $(X_i, X_j)$ since we could have enforced arc-consistency on $(X_k, X_i)$ with respect to illegal values in $D_i$. This represents transitive arc-consistency relations.
\end{itemize}

Hence, arc-consistency acts as a form of local repeated constraint propagation by enforcing it after every assignment. This way, we prune as much as of the domain space as possible by removing values that will possibly lead to illegal assignments.

\begin{algorithm}
\caption{Arc-Consistency Algorithm AC-3}\label{euclid}
\begin{algorithmic}[1]
\Procedure{AC-3}{$csp$} \textbf{returns} $false$ if inconsistency found and $true$ otherwise
\State $queue \leftarrow$ all arcs in $csp$
\While{$queue$ is not empty}
$(X_i, X_j) \leftarrow$ Remove-First($queue$)
\If{Revise($csp, X_i, X_j$)}
\If{size of $D_i$ = 0} \Return $false$ \EndIf
\For{each $X_k \in$ $X_i$.neighbours - $\{X_j\}$}
\State add $(X_k, X_i)$ to $queue$
\EndFor
\EndIf
\EndWhile
\EndProcedure
\\
\Procedure{Revise}{$csp$} \textbf{returns} $true$ if the domain $D_i$ of $X_i$ is revised
\State $revised \leftarrow false$ 
\For{each $x \in D_i$}
\If{no value $y_j \in D_j$ allows $(x_i, y_j)$ to satisfy the constraint between $X_i$ and $X_j$}
\State delete $x$ from $D_i$
\State $revised \leftarrow true$
\EndIf
\EndFor 
\EndProcedure
\end{algorithmic}
\end{algorithm}

The AC-3 algorithm is one that allows us to ensure arc-consistency via constraint propagation. It first begins with arcs from the CSP problem, and prunes values from each domain $D_i$ for the variable $X_i$ if there are any values such that $X_i$ is not arc consistent with any $X_j$. Once we do this, we re-enforce transitive arc-consistency by re-evaluating the values in the domains of the other neighbours of $X_i$, $X_k$ such that once $(X_i, X_j)$ is arc-consistent after the Revise procedure, each of the other neighbours of $X_i, X_k$ needs to be arc-consistent with the revised domain of $X_i, D_{i}^{'}$.\\

Notice that for a CSP problem, there are at most $n^2$ directed arcs in a complete graph where very node has a directed edge to other nodes. Additionally, each $X_i$ has at most $d$ values in their respective domains $D_i$, and hence on revising of neighbouring arcs, an arc $(X_i, X_j)$ can be re-queued for the Revise operation at most $d$ times(exactly $d$ if everytime we only delete 1 element from the $d$ elements in $D_i$). Each revise operation compares each cartesian product of the domains $D_i, D_j$ and hence incurs at most $d^2$ comparisons. Putting all of these together, we have that the AC-3 algorithm incurs a total time complexity of $O(n^2d^3)$.\\

Hence, we can use the AC-3 algorithm as a routine on our CSP to ensure that all values in each domain is arc-consistent since it enforces transitive constraint propagation. In particular, we can re-execute this routine after every assignment to maintain the arc-consistency, and this entire process is known as \textbf{Maintaining Arc-Consistency} procedure. This ensures that after every assignment, we revise our domains to contains values that only lead to legal assignments.\\

Notice that the AC-3 algorithm is used to maintain transitivity of arc-consistency on \textit{binary constraints}. In the case of ternary constraints where more variables are involved, we need to decompose the ternary constraints into multiple binary constraints if we wish to continue using the AC-3 algorithm. However, it is not always that ternary constraints can be decomposed into binary constraints, especially when constraints start to get complicated. Fortunately, we can extend the AC-3 algorithm from enforcing 2-consistency between pair of arcs $(X_i, X_j)$ to a general algorithm that establishes $k$-consistency on ternary constraints.

\subsection{Local Search on CSP}

Previously, we have discussed how to perform standard search over the state space for CSPs, by starting with an empty assignment, and assigning a value to each variable at each level of the search tree. We then improved this to ensure we only assign legal values by performing forward-checking, in particular checking arc-consistency between variables to reduce search space to only valid assignments instead of searching down paths that lead to invalid assignments.\\

However, we can also consider local search techniques for solving CSPs, by first starting with an initial assignment(which can obviously be allowed to violate constraints, or else we are done), and perform iterative improvement by applying modification operators to move the current configuration towards a valid assignment. Often, these modification operators aims to reduce the number of inconsistencies in the constraints, and hence called \textbf{heuristic repair} methods. One commonly used heuristic that measures such inconsistencies in constraints is called the $min-conflicts$, which measures the $number of constraints that are violated$. Hence, by seeking a solution that minimizes this heuristic, we are aiming to find a solution that satisfies the maximal number of constraints possible.

\subsubsection{Min-Conflicts}


\begin{algorithm}
\caption{Min-Conflicts}\label{euclid}
\begin{algorithmic}[1]
\State $current \leftarrow$ assignment for $csp$
\For{$i = 1$ to $max\_step$ }
\If{$current$ is solution for $csp$}
\Return $current$
\EndIf
\State $var \leftarrow$ randomly chosen variable from $csp$.Variables
\State $value \leftarrow$ value $v$ for$var$ that minimizes $conflicts(var, v, current, csp)$
\State $current.var \leftarrow value$
\EndFor
\end{algorithmic}
\end{algorithm}

Notice that we execute the Hill-Climbing search with min-conflicts as the heuristic function that we aim to minimize. Similar to Hill-Climbing search, we may face the issue of being stuck in a local minima that is not a global minima if the algorithm no longer finds any moves that can improve the current position, since it merely aims to reduce the heuristic function that leads to a better position now, but not in the long run and never explores nor backtracks. But surprisingly, in many cases, we are able to find the optimal solution with Hill-Climbing search with high probability.\\

Additionally, we can add element of exploration and exploitation by labelling constraints that are important, or have not been 'repaired' for a long time. We do this by assigning weights to constraints, which value can fluctuate over the period of the search. At each state of the search space, we aim to find an action that leads to a configuration that reduces the $total weight$ of all constraint violated. Depending on how we define the value adjustments, this allows us to get out of local minima by marking constraints that are stagnant for long time, or label constraints that absolutely need to be satisfied among all constraints.\\

An important feature of local search is that it allows flexibility in time, by providing us with the best known solution at the time of termination, even if the search is not complete. This is particularly useful for extremely large search spaces. Additionally, it allows for existing solutions to be fixed due to change in constraints by using the current previously valid assignment, and running local search on it with new constraints. 

\subsection{Structured CSP}

In general, solving CSPs is considered a computationally intractable problem. For CSPs with discrete valued variables, the search space is in the factor of $O(d^n)$ for all $n$ variables. To reduce this computation time, we can break CSPs into independent CSPs. For reduced CSPs with $c$ variables each, the computation time is reduced to $O(d^n) = O(d^c\frac{n}{c})$.\\

However, if we can find a substructure in the CSP, then we can exploit it to solve the CSP in polynomial time.

\begin{theorem}
If the CSP constraint graph is a tree, then an assignment to the CSP can be found in polynomial time in factor of $O(nd^2)$ time
\end{theorem}

To solve a tree-structured CSP, we first need to impose a hierarchical structure to the constraint graph. We do this by choosing an ordering of the graph that imposes \textbf{topological sorting} on the graph itself. Once that is performed, we can ensure arc-consistency of each arc $(X_i, X_j)$ for each in the tree, from the leaf node $X_j$ up to the parents. Since a tree with $n$ nodes contains at most $n-1$ arcs, that means that the total time incurred for ensuring arc-consistency on all arcs is $O((n-1) \times d \times d) = O(nd^2)$.\\

\begin{lemma}
If all arcs $(X_i, X_j), \forall$ i, j s.t $X_j$ $\in$ children of $X_i$ are consistent, then the following is true:

\begin{itemize}
\item There is legal assignment for CSP if $D_i  \neq \emptyset$ for each parent $X_i$ 
\item There is no legal assignment for CSP if $\exists D_i : D_i = \emptyset$
\end{itemize}
\end{lemma}

The second part of the claim is easy to see: If one of the domains $D_i$ is reduced to an empty set using arc-consistency, that means that for every variable in the domain $D_i$, it violates a constraint between $X_i$ and at least one child $X_j$. This means that there is no legal assignment for $X_i$, and hence for the CSP itself.\\

If every domain is non-empty, the algorithm ensures that every arc between a parent and child node $(X_i, X_j)$ is consistent. This means that for every value in the domain $D_i$ for $X_i$, there is legal assignment for each child $X_j$. Notice that although the algorithm does not ensure that the reverse is true: the arc $(X_j, X_i)$ is arc-consistent, we argue that the resulting reduced domains is valid. This is because while we ensure arc-consistency from the bottom of the tree up to the root, we assign values to variables by propagating down the tree. This means that we pick values from parent nodes \textit{before} the child nodes, we only have to ensure that the values we pick for the parent nodes $X_i$ induces valid values for the child node $X_j$ that is selected after the parent node, which is exactly what arc-consistency on $(X_i, X_j)$ ensures. \\

This also means that there is no need for backtracking when we assign values, since arc-consistency ensures that whatever values that we select for the parent nodes ensures that there is at least one valid value to be assigned to the child. Additionally, since the graph is a tree, each node only has at most one parent. Hence, the only values that can affect the selection of values for the current variable is the value of the parent node selected before. But because we ensured that each arc from parent to child node is arc-consistent, we know that we always have a valid value(is the domain is non-empty) for the current node no matter what the parent node is assigned. 

\begin{lemma}
Ensuring arc-consistency in a constraint graph that is a tree does not remove options for a valid CSP assignment
\end{lemma}

Notice that by ensuring arc-consistency, we only remove values from a domain $D_i$ if $\exists X_j : X_i = d \rightarrow D_j = \emptyset$.  That is, the value $d$ causes a child $X_j$ to have only assignments that violate constraints. For a valid assignment for a CSP, every value $d_i$ for each node $X_i$ is valid. Assume that a value $d^*_j$ for the variable $X_j$ is removed by the arc-consistency algorithm. That means that $\exists X_i$ such that all $d_i \in D_i$ are invalid assignments if $d_j$ is chosen.  But that also means that the value $d^*_i \in D_i$ from the valid assignment is also removed by the arc-consistency algorithm, implying that if $d^*_j$ is chosen, $d^*_i$ will be an invalid assignment for $X_i$ that violates some constraint. But this contradicts the fact that $d^*_i, d^*_j$ is in the valid assignment since having both violates at least 1 constraint in the CSP. Hence, options for a valid assignment for the CSP will never be removed because for each arc $(X_i, X_j)$, the value for the valid assignment $d^*_i$ will at least have the valid value in the assignment $d^*_j$ chosen for $X_j$, since both in the assignment implies they do not violate any constraints.\\

This brings us to our next lemma:

\begin{lemma}
If a valid assignment is possible for the CSP, it will always be found regardless of the topological ordering induced on the constraint graph
\end{lemma}

Observe that the valid assignment for the CSP is independent of the topological sorting of the constraint graph nodes. If we switch the topological ordering to form a new tree, the valid assignment values for CSP is still valid because picking each of the values $_i$ for variable $X_i$ in this valid assignment implies that for each of the new children $X_j$ of the reformed tree, we can minimally pick the valid values $d_j$ from the valid assignment since these combination of values do not yield violating constraints. By the lemma before, this means that this valid assignment is not removed from by the arc-consistency algorithm, and hence can still be found if the topological ordering is different. Although, it is possible that a different valid assignment may be found instead.
\pagebreak

\section{Reinforcement Learning}

Previously, we have discussed games in search trees using utility functions to determine which moves are the most optimal, and then propagating them back up using the Minimax and alpha-beta pruning algorithm. However, this requires that we be able to define the labelling for these outcomes of games. In some games, this may not be possible due to the sheer number of possible winning outcomes, or just that we do not perfectly know what is the best or most optimal outcome. In this case, it is difficult for the algorithm to search optimally without feedback from us. Hence, \textit{the agent needs to be able to function without perfect feedback}. Instead of telling the algorithm explicitly which path is most optimal, we instead incentivize algorithms to take better paths by providing \textbf{rewards or reinforcements} to it, which is a signal to the algorithm that a particular action is favored over others.\\

We also impose the property of \textbf{Markov Decision Process} in reinforcement learning, where \textit{the next state in the game is determined by the current state and actions, and not the entire sequence}.\\

The \textbf{policy} $\pi$ that the agent choose determine the agent's behaviour and what actions it take at each state. The policies can be:

\begin{itemize}
\item \textbf{Deterministic:} $a_t = \pi(s_t)$. The agent chooses the best valued action at the current state
\item \textbf{Stochastic:} $\pi(a | s_t) = \Pr[a_t = a | s_t]$. The policy is a distribution of actions that can be taken by the agent at the state in time. 
\end{itemize}

In order to determine what actions to take, the agent need to evaluate if the action, and possibly the sequence of actions taken after it will yield optimal rewards. Consider the reward function $r_t(a_t, s_t)$ at the time $t$ that infers the rewards received by the algorithm when taking the action $a_t$ at state $s_t$. Then, we see the value function as:  

\begin{equation*}
V^{\pi}(s_t) = r_t(a_t, s_t) + \gamma r_{t+1}(a_{t+1}, s_{t+1}) + ... = \sum^{\infty}_{l = 0} \gamma^l r_{t+l}(a_{t+1}, s_{t+1})
\end{equation*}

That represents the rewards that will be received by the algorithm by taking a sequence of action from the current state on subsequent resulting states, where $0 \leq \gamma \leq 1$ is known as the discount value. This discount value can be thought of as the \textit{importance} of the rewards received upon reaching the state $s_t$. A high value of $\gamma$(e.g = 1) would mean that the agent is long-sighted: the rewards received in the next state is almost as, or equally important as the the rewards received in future states. However, notice if the value of $\gamma$ is low, the value of $\gamma^l$ decays much quicker. This means the agent is more concerned with maximizing immediate rewards rather than future rewards.\\

We wish to choose a value maximizing policy based on the above value function. Notice that we are missing information to do so, which are rewards of unobserved states, which are usually non-terminal states. Hence, we have to use rewards inferred from current and transitioned states to learn the expected utility that is associated with each non-terminal state:

\begin{equation*}
U^{\pi}(s_t) = \E[V^{\pi}(s_t)] = \E[\sum^{\infty}_{l = 0} \gamma^l r_{t+l}(a_{t+1}, s_{t+1})]
\end{equation*}

Note that the value function \textit{has nothing to do with the agent's internal state of the environment}. It is fully objective, and aims to represent the rewards that the agent receives if it takes an action that transitions it to another state.

\subsection{Markov Decision Processes}

To define a Markov Decision Process(MDP), we first start with a fully-observable environment, such as the Tic-Tac-Toe game. In such cases, the agent is able to observe the entire environment during each of it's states, and also how it's actions taken affect the environment. In this case, as previously discussed, the agent would be able to construct an optimal strategy that reaches the goal state.\\

However, now let us look at the possibility that the \textit{environment interferes with the agent's actions}. That is, every action the agent performs happens with a probability of $p \leq 1$, and with a total probability of $1-p$ the environment could cause the agent to not be able to carry out it's intended action. In this case, the \textbf{transition model} is no longer deterministic that specifies the action taken at state $s, T(s, a, s')$, but rather in terms of probability:

\begin{equation*}
P(s' | a, s)
\end{equation*}

which denotes the probability that the agent indeed reaches the state $s'$ from $s$ by taking the action $a$, should the environment not interfere successfully. Now our transition model becomes stochastic. Additionally, we assume that the transition model is \textbf{Markovian}: the odds of the agent transitioning to a state $s'$ from the state $s$ by the action $a$ is affected \textit{only by where it's current state $s$ is}. The states that the agent were in in the past, $s_1, ..., s_{n-1}$ do not affect the probabilities of transition from it's current state to the next state. More formally:

\begin{equation*}
P(s' |a, s_n, s_{n-1},..., s_1) =  P(s' |a, s_n)
\end{equation*}

Lastly, the utility function for the agent is now dependent on the \textbf{reward function} $R(s)$ at state $s$. Unlike previous strategies, the reward function is now cumulative: it depends on the sequence of actions that the agent has taken. For example, the total utility that the agent receives using a policy $\pi$ could just be the sum of the rewards at each state it has visited. A sequential decision problem for a fully observable, stochastic environment with a Markovian transition model and additive rewards is called a \textbf{Markov Decision Process(MDP)}.

\subsubsection{Optimal Policy}

Since we no longer have a deterministic transition model, a fixed action sequence that is used for a deterministic version of the search may or may not yield the best results. Hence instead of employing the strategy that we discussed in zero-sum games, we now make use of a \textbf{policy} $\pi$, which is a function that tells the agent the action that it \textit{should} take at state $s$. Hence, the policy function at a state $s, \pi(s)$, explicitly determines the agent function.\\

 Notice that since the environment is now stochastic in nature, running the agent with a fixed policy may return multiple different environment histories. Hence, the quality of a policy is no longer determined by the utility of a single environment history, but rather the \textbf{expected} utility from a series of environment histories returned by running the same policy on the environment several times. The optimal policy, is one that yields the \textbf{highest expected utility} out of all policies.\\
 
 Additionally, it is also important to note that the reward function $R(s)$ influences the policy greatly, and how conservative it is. If rewards for reaching the goal state is low and penalty elsewhere is high, then the agent will tend to be more conservative.
 
 \subsubsection{Learning Optimal Policies}
 
We have seen that the optimal policy is one that gives the highest expected returns for the agent at any state in the environment. However, we can only pick an optimal policy for the agent if we actually know the true probability distribution for the MDP, $\Pr(s' | a, s)$. Usually, in many real world problems, such a distribution is unknown to us, and hence we required the agent to \textit{learn} this optimal strategy.\\
 
 Remember that in Reinforcement Learning, we aim to maximize the utility that the agent gets through it's value function:
 
 \begin{equation*}
V^{\pi}(s_t) = r_t(a_t, s_t) + \gamma r_{t+1}(a_{t+1}, s_{t+1}) + ... = \sum^{\infty}_{l = 0} \gamma^l r_{t+l}(a_{t+1}, s_{t+1})
\end{equation*}

where $\gamma$ is the discount factor that determines how much the agent values long term rewards. If $\gamma = 1$, then

 \begin{equation*}
V^{\pi}(s_t) = r_t(a_t, s_t) + r_{t+1}(a_{t+1}, s_{t+1}) + ... = \sum^{\infty}_{l = 0} r_{t+l}(a_{t+1}, s_{t+1})
\end{equation*}

are known as additive rewards. In this case, rewards in the future are as valuable as the immediate rewards. However, due to the possibly stochastic nature of environments, we may not always know what rewards that we will get by taking an action $a$. Instead, we wish to compute the expected utility:

\begin{equation*}
U^{\pi}(s_t) = \E[V^{\pi}(s_t)] = \E[\sum^{\infty}_{l = 0} \gamma^l r_{t+l}(a_{t+1}, s_{t+1})]
\end{equation*}

where $s_t$ is the state that the agent is in at time $t$, following the actions that policy $\pi$ recommends. Observe that if the policy is deterministic, then clearly $U^{\pi}(s_t) = V^{\pi}(s_t)$. Then, computing the optimal policy $\pi^*$ is given as follows:

\begin{equation*}
\pi^* = {\arg \max}_{\pi} U^{\pi}(s_0)
\end{equation*}

Where the optimal policy aims to provide the the best action starting at the current state $s_0$ that maximises \textbf{long term} expected rewards, as opposed to $r(a, s)$ that indicates short term rewards. Note also that as the discount factor $\gamma$ goes to 0, $U(s)$ goes closer to $r(a, s)$. Additionally, because of the Markov property, when we start at a state, say $s_0$, and move onto the state $s_1$, the agent no longer needs to consider the previous state $s_0$. Hence, the utility function $U(s)$ allows the agent to select the best action using the principle of best expected utility at state $s$:

\begin{equation*}
\pi^*(s) = {\arg \max}_{\pi} U^{\pi}(s) = {\arg \max}_{a \in A(s)} \sum_{s'} \Pr(s' | s, a) U(s') 
\end{equation*}

\subsection{Bellman Equation}

We have seen that the utility of a state is the expected sum of discounted rewards from that state onwards. From this, we can break the equation down further using the following reasoning: \textit{the utility of a state is the immediate reward obtained from that state, followed by the expected discounted reward from the next state}.

\begin{equation*}
\begin{aligned}
U^{\pi}(s_t) &= \E[\sum^{\infty}_{l = 0} \gamma^l r_{t+l}(a_{t+1}, s_{t+1})] \\
&= \E[r_t(a_t, s_t) + \gamma r_{t+1}(a_{t+1}, s_{t+1}) + \gamma^2 r_{t+2}(a_{t+2}, s_{t+2})  + ...] \\
&= \E[r_t(a_t, s_t) + \gamma (r_{t+1}(a_{t+1}, s_{t+1}) + \gamma r_{t+2}(a_{t+2}, s_{t+2})  + ...)] \\
&= \E[r_t(a_t, s_t) + \gamma \sum^{\infty}_{l = 1} \gamma^l r_{t+l}(a_{t+1}, s_{t+1})] \\
&= \E[r_t(a_t, s_t)] + \gamma U^{\pi}(s_{t+1})
\end{aligned}
\end{equation*}

The optimal policy would then be one that maximizes:

\begin{equation*}
\pi^*(s) = {\arg \max}_{\pi} U^{\pi}(s) = {\arg \max}_{a \in A(s)} \{ r(a, s) + \gamma U^{\pi}(T(a, s))\} 
\end{equation*}

with a optimal utility of: 

\begin{equation*}
\begin{aligned}
U^{\pi^*}(s_t) &= r^*_t(a_t, s_t) + \gamma U^{\pi^*}(s_{t+1})
\end{aligned}
\end{equation*}

As in with MDP planning, we could formulate and compute the optimal policy $\pi^*$ \textit{if we knew the reward function $r(a, s)$ and the transition model $T(a, s)$}. However we don't, and therefore we cannot explicitly compute the optimal policy if we do not know what the rewards are if we take an action and what the next state is. Instead, we try to learn a policy that \textit{converges} to the optimal one.

\subsection{Q-Learning}

Previously we have seen that computing and even learning the utility functions may not always be possible, as we require knowledge of the reward and the transition model. Instead, we learn an action-utility representation known as \textbf{Q-Learning}, denoting $Q(s, a)$ as the function that returns the value of performing action $a$ at state $s$. More formally:

\begin{equation*}
\begin{aligned}
Q^{\pi}(s, a) = r(s, a) + \gamma U^{\pi}(s' = T(s, a)) 
\end{aligned}
\end{equation*}

is the utility the agent obtains by performing action $a$ at state $s$(which may or may not be the one recommended by policy $\pi$), and then performing the actions recommended by policy $\pi$ from state $s'$ onwards. What we wish to learn is the optimal Q function. That is, the Q function that maximizes the possible rewards if an action $a$ is taken. Using the Bellman equation, we have that:

\begin{equation*}
\begin{aligned}
Q^*(s, a) &= r(s, a) + \gamma U^{*}(s' = T(s, a)) \\
&= r(s, a) + \gamma  \sum_{s'} \Pr(s' | s, a) \max_{a'} Q^*(s' ,a')
\end{aligned}
\end{equation*}

$Q^*(s,a)$ represents the maximum utility that the agent can receive from state $s'$, given that the agent has taken the action leading to $s'$. Observe further that the Q function and the utility function are related as follows:

 \begin{equation*}
\begin{aligned}
U^*(s) = \max_{a} Q^*(s ,a)
\end{aligned}
\end{equation*}

Because the maximum utility that the agent can receive at the next state $s', U^*(s)$, will be the maximum utility given an action $a'$ at $s'$ is taken. Hence, at every state we can replace the utility function $U^*(s)$ with the Q function.\\

Notice that the Q-Learning function does not require knowledge of the transition model $\Pr[s' | s, a]$: it gains this knowledge from learning or from observation by actually transitioning to the state by taking the action. Hence, it is a \textbf{model-free} method. Therefore, we have expressed the learning of our utility function in terms of a function that can observe these values without having to know the transition and reward functions.

\subsection{Value-Iteration Algorithm}

The Value-Iteration Algorithm aims to update the Q function values iteratively by observing the rewards and transitioned states as the agent performs actions. 

\begin{algorithm}
\caption{Value-Iteration Algorithm}\label{euclid}
\begin{algorithmic}[1]
\State $\hat{Q}(s,a) \leftarrow 0, \forall a$
\State $s \leftarrow s_0$ 
\For{$t = 0,1,...$}
	\State $a \leftarrow \small{\textsc{SELECT-ACTION}}(s)$
	\State $r \leftarrow r(s, a)$
	\State $s' \leftarrow T(s, a)$
	\State $\hat{Q}(s, a) \leftarrow r + \gamma \max_{a'} \hat{Q}(s', a')$
\EndFor
\end{algorithmic}
\end{algorithm}

Let us assume that the rewards $r(s, a)$ are non-negative. Observe that in the beginning, all values of $\hat{Q}(s, a) = 0$, and $\hat{Q}(s, a) \leq Q^*(s, a)$. As we continue to observe the rewards and updates the values of $\hat{Q}(s, a)$, the value of $\hat{Q}(s, a)$ gets updated to the actual rewards received when action $a$ is taken at state $s$. Because we observe and update the exact values of rewards, the value of $\hat{Q}(s, a)$ \textit{never overestimates} $Q^*(s, a)$, and converges to $Q^*(s, a)$ over time as the actual values of the reward are observed by the agent.

\subsubsection{Policy Update}

Following the general algorithm, we have that the A values are constantly updated by new values as the agent observes rewards. This seems like a good idea, but notice that it completely overwrites the values of the previous $Q(s, a)$ with the new values. Complete ignoring the previous values in the computation of $Q(s, a)$ can result in large fluctuations of the Q values and lead to an underestimate of the actual utility.\\

Instead, we aim to stabilize the update of Q values by using a form of \textit{learning rate}. As we observe the values of $Q(s, a)$ more and more times, we aim to incorporate the previous values seen and decrease the contribution of the new value to $Q(s, a)$:

\begin{equation*}
\hat{Q}(s, a) \leftarrow (1 - \alpha_t)\hat{Q}(s, a) + \alpha_t(r(s, a) + \gamma \max_{a'} \hat{Q}(s', a'))
\end{equation*}

where 

\begin{equation*}
\alpha_t = \frac{1}{1 + N[s, a]}
\end{equation*}

represents a fraction over the number of times we have visited the state. As we continue to observe more values, we wish to incorporate contributions of each value observed, and hence decrease contribution of new values to stabilize the updates.

\subsubsection{\textsc{SELECT-ACTION}}

In the Value-Iteration Algorithm, the action $a_t$ is selected based on a routine \textsc{SELECT-ACTION}. However, how do we specify which action to select at time $t$ and state $s_t$? We can perform the selection deterministically, such as specifying a criteria(e.g action with best expected rewards) using greedy selection, or incorporate randomization. One way to do this is the following:

\begin{equation*}
\Pr[a | s] = \frac{e^{\epsilon \hat{Q}(s, a) }}{\sum_{a'} e^{\epsilon \hat{Q}(s, a) }}
\end{equation*}

where $\epsilon$ is a tunable parameter, represents the probability of being picked. Observe that we pick actions with higher expected utility value of $Q(s, a)$ with a higher probability. The sensitivity to Q values are influenced to the value of $\epsilon$. Low values of $\epsilon$(e.g 0) indicate that probabilities do not different too much with increasing values of $Q(s, a)$, with $\epsilon = 0$ being complete randomness. On the other hand, higher values of $Q(s, a)$ indicate biasedness towards high expected utilities, and we want the agent to be greedier.

\subsection{Regret Minimization}

Previously, we have mentioned that how we select which action to pick at each stage is critical for for update policy to converge to the optimal policy.  What we wish to do in selecting the right action, is to pick the action that does not fall too far from the optimal action. That is, if we perform a series of actions over time $T$, we want that our expected rewards of picking actions $a$ to not fall too far from the optimal utility, $U^*$. More formally, let

\begin{equation*}
U^*(s) = Q^*(s, a^*) = \max_{a \in A(s)} Q^*(s, a)
\end{equation*}

again be the optimal utility achievable at state $s$ by executing the optimal policy $\pi^*$. Note also again that we do not know what $\pi^*$ is, else we could compute it. Then, let $\pi$ be some other policy that may not be the optimal. The policy assigns the probability $p^t_i$ to the action $a^t_i$ at time $t$. Then, the expected utility of the agent at time $t$ following the policy $\pi$ is given by:

\begin{equation*}
u^{\pi}_t = \E[r(s_t, a_t)] = \sum_i p^t_i u^t_i
\end{equation*}

And the expected utility of a policy $\pi$ is given by:

\begin{equation*}
U^{\pi}(s) = \sum^{\infty} u^{\pi}_t = \E[Q^{\pi}(s, a)]
\end{equation*}

which can also be represented by the expectation over the rewards of it's action-value representation in the form of the Q function we saw previously. Remember we aim to learn a policy $\pi$ that maximize our cumulative value - then this policy should be one that is not too far from the optimal value $U^*$. More formally, we aim to reduce:

\begin{equation*}
\frac{1}{T}(U^*_T - U^{\pi}_T)
\end{equation*}

which is the difference between the expected value that we get from a policy $\pi$, and the optimal policy. This is known as \textbf{regret}. We see that the probability distribution that we place on the \textsc{SELECT-ACTION} routine, influences the expected utility that the agent gets from the updated Q function. Bad action selection could lead to high underestimation of the actual optimal value that we could have received.

\subsubsection{Greedy Algorithm}

How we can pick an action deterministically, is to \textit{greedily select an action that maximizes the total reward up to time $t$}. That is, we maintain a set of actions $S_t$ that maximizes the total rewards from previous actions. Using a deterministic tie-breaking scheme, we pick an action from this set of actions.\\

On paper, this would mean that we would pick an action at time $T$ is likely to get the highest expected rewards. However, we shall soon see that for any \textit{deterministic} action-selection routine, we can always find a set of inputs such that it gets very bad rewards.

\begin{theorem}
For any deterministic algorithm $A$, there is a sequence of actions for which $U^*_T \geq \ceil{\frac{(n-1)\times T}{n}}$, but $U^A_T = 0$
\end{theorem}

\textit{Proof:} If the algorithm is deterministic, and we know how the algorithm works, given a certain input, we can determine what action $a_{t-1}$ the algorithm will pick at the next step. In that case, given an online learning setting where we can provide the input at each step, we can always construct an input where the action that the algorithm will pick will yield 0 rewards, and all other actions will yield 1.\\

In general, we can construct an input for a sequence of actions up to time $T$, such that the algorithm always gets 0 rewards at each time step but all other actions get 1, and hence after time $T$, $U^A_T = 0$. Given $n$ actions, that means that at each round there are $n-1$ 1s associated to the other non-selected actions. Over time $T$, there are a total of $(n-1) \times T$ 1s, and hence an average of $\frac{(n-1) \times T}{n}$ 1s. But that means that \textit{at least} 1 action would yield a utility of $U_T \geq \frac{(n-1) \times T}{n}$ 1s.

\subsubsection{Randomized Greedy Algorithm}

Stepping up from the greedy algorithm, we combat the adversarial aspect by introducing randomness. That is, the only change that we make is instead of a deterministic tie-breaking scheme in selection of actions from $S_t$, we instead pick actions from this set randomly with uniform probability. 

\begin{algorithm}
\caption{Randomized Greedy Selection}\label{euclid}
\begin{algorithmic}[1]
\For{$t = 0,1,...$}
	\State $U^{\max}_t \leftarrow \max_i U^i_t$
	\State $S_t \leftarrow \{ i \in N : U^{i-1}_t = U^{\max}_t\}$
	\For{$i \in N$}
	\If{$i \in S_t$}
		\State $p^i_t \leftarrow \frac{1}{|S_t|}$
	\Else
		\State $p^i_t \leftarrow 0$ 
	\EndIf
	\EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

That is, at each round we pick each action that is in the set $S_t$ with uniform probability, and all other actions with probability 0. In fact, with some observations, we will see that the Randomized Greedy algorithm can lose at most $\log n + 1$ times between the loss of the best action. \\

First, observe that as the time $t$ passes, actions are removed from the set $S_t$ as these actions incur losses. It is also true that actions can also be added to the $S_t$, however, this actually lowers the probability of picking a losing action and does not provide a good bound on the number of losses Randomized Greedy will make.

Consider the number of times $l$ that the best action loses. That is, the best action does not yield rewards at times $t_1,..., t_l$. Then we wish to ask the question: in between losses $t_j$ and $t_{j+1}$, how many times will the Randomized Greedy selection lose? Let the number of times that the algorithm loses be $k_j$. Then, we can observe that if there is a single time period where there are $k_j$ actions that lost, then there is $\frac{k_j}{|S_t|}$ probability that the agent picks a losing action:

\begin{equation*}
\E[U^*_{t_j} - U^{RG}_{t_j}] = \frac{k_j}{|S_t|}
\end{equation*}

Furthermore, observe that over a period of time between $t_j$ and $t_{j+1}$, if actions are slowly removed from $S_t$(in particular, removed 1 at each round), then:

\begin{equation*}
\begin{aligned}
\E[U^*_{t_j} - U^{RG}_{t_j}] &= \frac{1}{|S_{t_j}|} + \frac{1}{|S_{t_j}| - 1} + ... + \frac{1}{|S_{t_j}| - k_j + 1} \text{ (actions removed 1 at a time)}\\ 
& \geq \frac{l_1}{|S_{t_j}|} + \frac{l_2}{|S_{t_j}| - l_1} + ... + \frac{l_k}{|S_{t_j}| - l_{k-1}} \text{ (actions removed $l_i$ at a time)}\\
&=   \underbrace{\frac{1}{|S_{t_j}|} + ...}_{l_1 \text{ times}} + \frac{1}{|S_{t_j}| - l_1} + ... \\
& \geq \underbrace{\frac{1}{|S_{t_j}|} + ...}_{k_j \text{ times}} = \frac{k_j}{|S_t|} \text{ (actions removed all at once)}
\end{aligned}
\end{equation*}

We can see that removing actions once at a time without having actions added into the set , yields the greatest loss. In fact, it acts as an upper bound for the number of times that the Randomized Greedy algorithm will lose, which is further upper bounded by:

\begin{equation*}
\begin{aligned}
\frac{1}{|S_{t_j}|} + \frac{1}{|S_{t_j}| - 1} + ... + \frac{1}{|S_{t_j}| - k_j + 1} & \leq \log|S_t| + 1\\
&= \log n + 1
\end{aligned}
\end{equation*}

Hence, the Randomized Greedy algorithm loses at most times in logarithmic factor to the number of actions available in between losses of the best action. Then, in total, the number of losses the Randomized Greedy algorithm will make is:

\begin{equation*}
\log n + (\log n + 1 )\times m
\end{equation*}

where $m$ is the number of losses the best action makes up to time $t$.

\subsubsection{Multiplicative Weights Update}

Now, we have an algorithm that does not lose out too much from the optimal utility,  with a logarithmic factor. However, it turns out that we can actually do better than this by adjusting the probabilities with which we select actions. With Randomized Greedy selection, the algorithm does not use knowledge of how good a move is. But, if we adjust probabilities to pick better moves with higher utilities, we can actually achieve close to the utility of the best action.

\begin{algorithm}
\caption{Multiplicative Weights Update}\label{euclid}
\begin{algorithmic}[1]
\For{$n = 1,.., n$}
\State $w^i_1 \leftarrow 1$
\State $p^i_1 \leftarrow \frac{1}{n}$
\EndFor
\For{$t = 0,1,...$}
	\For{$i = 1,.., n$}
		\State $w^i_t \leftarrow w^i_{t-1} \times e^{\epsilon u^i_t}$
	\EndFor
	\State $W_t \leftarrow \sum_i w^i_t$
	\For{$i = 1,..., n$}
		\State $p^i_t \leftarrow \frac{w^i_t}{W_t}$
	\EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

The weights used in the algorithm $w^i_t$ represents how good the moves are with respect to the total utility that the action has earned up to time $t$. If the action yields positive rewards at time $t$, then $v^i_t$ is positive and hence $w^i_t$ increases in the update step. Then, each move with picked with respect to probability that scales with it's weight at time $t$.\\

More importantly, we shall see that this algorithm yields rewards that are \textit{at least as good as} the best action. Firstly, we have that the weights with respect to time $t + 1$ is:

\begin{equation*}
\begin{aligned}
W_{t+1} &= \sum_i w^i_{t+1} \\
& \geq w^*_{t+1} \\
&= w^*_t \times  e^{\epsilon u^*_{t+1}} \\
&= w^*_1 \times e^{\epsilon u^*_{1}} \times ... \times e^{\epsilon u^*_{t+1}} \\
&= e^{\epsilon (u^*_{1} + ... + u^*_{t+1})} \\
&= e^{\epsilon U^*_{t+1}}
\end{aligned}
\end{equation*}

On the other hand, we also have that:

\begin{equation*}
\begin{aligned}
W_{t+1} &= \sum_i w^i_{t+1} \\
&= \sum_i w^i_t \times e^{\epsilon u^i_{t + 1}} \\
&\leq \sum_i w^i_t \times (1 + \epsilon u^i_{t+1} + (\epsilon u^i_{t+1})^2) \text{ using the fact that for $x \in [-1, 1], 1 e^x \leq 1 + x + x^2$}\\
&\leq \sum_i w^i_t \times (1 + \epsilon u^i_{t+1} + \epsilon^2) \text{ since $0 \leq u^i_{t+1} \leq 1 \rightarrow (\epsilon u^i_{t+1})^2) \leq \epsilon^2$}\\
&= \epsilon \sum_i w^i_t u^i_{t+1} + (1 + \epsilon^2) \sum_i w^i_t \\
&= \epsilon \times W_t \sum_i p^i_t u^i_{t+1} + (1 + \epsilon^2) \sum_i w^i_t \text{ using the fact that $\frac{w^i_t}{W_t} = p^i_t$}\\
&= \epsilon W_t (u^{MWU}_{t+1}) + (1 + \epsilon^2) W_t \text{ because $u^{MWU}_{t+1} = \sum_i p^i_t u^i_{t+1}$ is the time $t+1$ expected reward of MWU}\\
&= W_t (1 + \epsilon^2 + \epsilon u^{MWU}_{t+1})\\
&\leq W_t (e^{\epsilon^2 + \epsilon u^{MWU}_{t+1}}) \text{ using the fact that $1 + x \leq e^x$}
\end{aligned}
\end{equation*}

Applying the equation above recursively, we then have:

\begin{equation*}
\begin{aligned}
W_{t+1} &\leq W_t (e^{\epsilon^2 + \epsilon u^{MWU}_{t+1}})\\
&= W_{t -1} \times (e^{\epsilon^2 + \epsilon u^{MWU}_{t}}) \times (e^{\epsilon^2 + \epsilon u^{MWU}_{t+1}})\\
& ...\\
&= W_1 \times  (e^{\epsilon^2 + \epsilon u^{MWU}_{1}}) \times  (e^{\epsilon^2 + \epsilon u^{MWU}_{2}}) \times ... \times (e^{\epsilon^2 + \epsilon u^{MWU}_{t+1}})\\
&= n \times e^{t\epsilon^2 + \epsilon(\sum^{t+1}_{j=1} u^{MWU}_{j})} \\
&= n \times e^{t\epsilon^2 + \epsilon U^{MWU}_{t+1}} 
\end{aligned}
\end{equation*}

Taking the first part and second part,

\begin{equation*}
\begin{aligned}
& e^{\epsilon U^*_{t+1}} \leq W_{t+1} \leq n + e^{t\epsilon^2 + \epsilon U^{MWU}_{t+1}}\\
& \epsilon U^*_{t+1} \leq \log n +  t\epsilon^2 + \epsilon U^{MWU}_{t+1} \\
& U^*_{t+1} \leq \frac{\log n +  t\epsilon^2}{\epsilon} + U^{MWU}_{t+1} \\
\end{aligned}
\end{equation*}

which shows that the expected rewards of the MWU algorithm is at least as good as the expected reward of the best action.

\section{Logical Agents}

Previously in search, agents had knowledge of the environment, transition models and the possible utilities at several states. However, such knowledge is limited. For example, the agent playing chess using purely these information, may not be able to identify that it cannot reach the checkmate position to win from it's current position. Meaning, it cannot \textit{infer} knowledge based on what it already knows. Instead, what we try to do is formulate smarter agents: Agents that are able to infer information based on an existing knowledge. What the agent currently knows is called a \textbf{knowledge base}. The knowledge base first starts off empty, but continue to grow as the agent makes new inferences, and the inferences made by the agent are \textit{sentences} in a \textit{formal language} in the knowledge base.\\

\subsection{Knowledge-Based Agents}

The difference between logical agents and the previous agents is that the logical agent has a \textbf{knowledge base} that tracks what the agent knows so far. The knowledge base consists of \textbf{sentences or axioms} that is expressed in a \textbf{knowledge representation language}. This knowledge base may contains background knowledge using prior knowledge or assumptions. A logical agent, very generally, can be formulated as such:

\begin{algorithm}
\caption{Knowledge-Based Agent}\label{euclid}
\begin{algorithmic}[1]
\State \textbf{Persistent:} $KB$ - a knowledge base, $t$ - time counter
\\
\State \textsc{TELL}($KB$, \textsc{MAKE-PERCEPT-SENTENCE($percept, t$)})
\State $action \leftarrow$ \textsc{ASK}($KB$, \textsc{MAKE-ACTION-QUERY($t$)})
\State \textsc{TELL}($KB$, \textsc{MAKE-ACTION-SENTENCE($action, t$)})
\State $t \leftarrow t + 1$
\\
\Return $action$
\end{algorithmic}
\end{algorithm}

The knowledge-based agent is governed by 2 actions: \textsc{TELL} and \textsc{ASK}. They justify the actions that the logical agent must be able to perform. Firstly, the agent must be able to take in new percepts. This is seen by performing the \textsc{TELL} operation when it receives a new percept through sensors, and performs an action using actuators. Both operations update the knowledge base based on the perceived new environment, or the new action performed. For example, in the current state the agent may perceive that it cannot make a certain legal move based on the rules that it knows, and hence updates it's knowledge base on this information.\\

Then, using the existing knowledge and newfound ones(if any), the agent performs an action using the \textsc{ASK} operation, which makes a legal and perhaps even optimal move based on what it already knows. In relation to Reinforcement Learning, RL actually plays a big role in the \textsc{ASK} operation, where it selects the optimal ones in the presence of multiple valid actions.\\

Inference may be present in both \textsc{TELL} and \textsc{ASK} operations, where the agent derives new sentences from existing ones and the current percept. Additionally, we required that the knowledge base be updated with information that is correct. That is, new axioms must indeed follow from what is implied by the current knowledge. This is also called \textbf{entailment}. For now, we are interested in the \textsc{TELL} operation: how do we infer new information based on existing information and new percepts?

\subsection{Logic}

The fundamental concepts of logical representation and reasoning that allow us to formulate sentences are as follows:

\begin{itemize}
\item The \textbf{syntax} of the knowledge representation language tells us how we can express logic in the form of sentences, as well as if a sentence is valid. For example, the arithmetic language tells us that $x + y = m$ is valid, but $x y +>$ is not.
\item The \textbf{semantics} of the language tells us the meaning of the sentences formulated in that language, as well as the truth values behind these sentences. $x+y=4$ would hold true in the arithmetic language where $x=2, y=2$, but false when $x=1, y=1$.
\item a model $m$ is said to \textbf{model} the logical sentence $\alpha$, if $\alpha$ is true under $m$. $m$ can usually be thought of as the 'world' or environment that the agent lives in. Following the previously example, $x + y = 4$ is satisfied by the environment where there are $x$ red balls and $y$ blue balls, and there are always a total of 4 balls. Since $x + y =4$ is always in such a world, we say that it is a model of $\alpha$.
\end{itemize}

\subsubsection{Entailment}

We say that a logical sentence, or a collection of logical sentences \textbf{entails} another, if the entailed sentence \textit{follows logically} from the sentence. More formally, we say that if $\alpha$ entails $\beta$, then:

\begin{equation*}
\alpha \models \beta
\end{equation*}

we say that $\beta$ follows logically from $\alpha$, or that $\alpha$ implies $\beta$. Following the property of models, let $M(\alpha)$ be the set of all models under which $\alpha$ is true. Then the following formal definition ensues for entailment:

\begin{definition}
$\alpha \models \beta$ if and only if, in every model $m$ in which $\alpha$ is true, $\beta$ is also true.
\end{definition}

That is, if $m \in M(\alpha)$, then $m \in M(\beta)$, and hence $M(\alpha) \subset M(\beta)$. Hence, we can see that if $\alpha \models \beta$, then $\alpha$ is a \textit{stricter} condition than $\beta$. \\

Then, using the definition of entailment, we wish that our inference algorithm be \textbf{sound}and \textbf{complete}. This ensures that our inference algorithm does indeed derive the optimal and correct solutions to our logic problem. Now, let the notation

\begin{equation*}
KB \vdash_A \alpha
\end{equation*}

denote that the logical sentence $\alpha$ is derived from the knowledge base $KB$ using inference algorithm $A$. Then, the inference algorithm $A$ is \textbf{sound} if:

\begin{equation*}
KB \vdash_A \alpha \rightarrow KB \models \alpha
\end{equation*}

That is, if a logical statement $\alpha$ is inferred by the algorithm $A$ from the knowledge base $KB$, then that statement $\alpha$ indeed follows from the logical sentences in the knowledge base. Our inference algorithm only infers what is indeed correct, and not nonsense. We also say that such an algorithm is \textbf{truth-preserving}.\\

The inference algorithm $A$ is \textbf{complete}, if it satisfies:

\begin{equation*}
KB \models \alpha \rightarrow KB \vdash_A \alpha 
\end{equation*}

If a logical statement $\alpha$ is true and follows from the logical sentences of the knowledge base, then it would be found by the inference algorithm $A$. Then, the agent would not having missing information using the inference algorithm $A$.

\subsection{Propositional Logic}

\subsubsection{Truth Table Enumeration}

One simple way to check if a sentence $\alpha$ is entailed by the knowledge base that we have, is to enumerate through the models in $M(KB))$ and check that $\alpha$ is indeed true for each model.\\

The algorithm is sound is because it simply uses the direct definition of entailment, and checks that the condition holds. It is also complete because given a finite number of symbols, then there are only a finite number of propositional logic statements. However, this still incurs a time complexity of $O(2^n)$ for $n$ symbols. Using a depth first search technique, this incurs a space complexity of $O(n)$.

\begin{algorithm}
\caption{\textsc{Truth-Table Enumeration}}\label{euclid}
\begin{algorithmic}[1]
\Procedure{\textsc{TT-ENTAILS?}}{$KB, \alpha$} \textbf{returns} $true$ or $false$
\State \textbf{inputs: } $KB$ - the knowledge base, $\alpha$ - sentence in propositional logic
\State $symbols \leftarrow$ a list of propositional symbols in $KB, \alpha$\\
\Return \textsc{TT-CHECK-ALL($KB, \alpha, symbol, \{\}$)} 
\EndProcedure
\\
\Procedure{\textsc{TT-CHECK-ALL}}{$KB, \alpha, symbols, model$} \textbf{returns} $true$ or $false$
\If{\textsc{EMPTY?}$symbols$}
\If{\textsc{PL-TRUE?}$(KB, model)$}
\Return \textsc{PL-TRUE?}$(\alpha, model)$
\Else \
\Return $true$
\EndIf
\Else
\State $P \leftarrow \textsc{FIRST}(symbols)$
\State $rest \leftarrow \textsc{REST}(symbols)$
\State \Return $\textsc{TT-CHECK-ALL}(KB, \alpha, rest, model \cup \{ P = true\})$ \textbf{and}\\  $\textsc{TT-CHECK-ALL}(KB, \alpha, rest, model \cup \{ P = false\})$
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsubsection{Propositional Theorem Proving}

Previously, we have shown how we can determine entailment through basic model checking: by enumerating all possible table values and checking that the definition of entailment always holds. However, this can be very expensive, and not leveraging our knowledge of propositional logic. Instead, what we shall do is that we can show entailment(or lack of thereof) through \textbf{theorem proving} - by applying the rules of inference directly to sentences.\\

First, we introduce a series of concepts related to propositional logic:

\begin{itemize}
\item \textbf{Logical Equivalence:} two sentences $\alpha$ and $\beta$ are logically equivalent, if they are true in the same set of models. For example $P \vee Q$ and $Q \vee P$ are logically equivalent. Then, we denote logical equivalence as:

\begin{equation*}
\alpha \equiv \beta
\end{equation*}

Alternatively, that means that if either of $\alpha$ or $\beta$ is true, then the other must be true. Hence, one actually entails the other. We can then look at equivalence as: 
\begin{equation*}
\alpha \equiv \beta \leftrightarrow \alpha \models \beta \land \beta \models \alpha
\end{equation*}

\item \textbf{Validity:} We say that a sentence is valid if it is true in \textit{all} models. For instance, $P\lor \lnot P$ is always valid, since it always holds. Valid sentences are also known as tautologies: they are necessarily true.

\item \textbf{Satisfiability:} A sentence is satisfiable if it is $true$ in \textit{some} model. Additionally, a sentence is \textbf{unsatisfiable} if it is never true: it is $false$ in all models.
\end{itemize}

Using the above concepts, we can derive the following deduction theorem:

\begin{theorem}
For any sentences $\alpha$ and $\beta$, $\alpha \models \beta \leftrightarrow (\alpha \implies \beta)$ is valid
\end{theorem}

And this is exactly the same formal definition that we defined earlier on entailment: a sentence $\alpha$ entails another $\beta$, if $\beta$ follows from $\alpha$ in all models. That is, $\alpha \implies \beta$ always holds for every model, which means that it is indeed valid.\\

Furthermore, notice that validity and satisfiability are linked: if a sentence $\alpha$ is valid, then it always holds in all models no matter what. That would imply that it's negation $\lnot \alpha$ can never hold in any model. That is, $\lnot \alpha$ is unsatisfiable. In fact that means that to show entailment, which originally is the following using Theorem 4:

\begin{equation*}
KB \models \alpha \leftrightarrow (KB \implies \alpha) \text{ is valid}
\end{equation*}

We can also show that the \textit{negation} of $(KB \implies \alpha)$ is unsatisfiable. That is, proving the above is logically equivalent to:

\begin{equation*}
KB \models \alpha \leftrightarrow (KB \land \lnot \alpha) \text{ is unsatisfiable}
\end{equation*}

\subsubsection{Inference Rules}

We now discuss some techniques known as inference rules that allow us to infer rules from existing ones, and eventually arrive at the a conclusion.\\

\textbf{Modus Ponens:} $\frac{\alpha \implies \beta, \alpha}{\beta}$\\

Says that if we have the rule $\alpha \implies \beta$, and we know the value of $\alpha$, then we can infer what $\beta$ is.\\

\textbf{And-Elimination:} $\frac{\alpha \land \beta, \alpha}{\alpha} \equiv \frac{\alpha \land \beta, \alpha}{\beta}$\\

And-Elimination tells us if we have a conjunction of clauses, if the conjunction holds true then we know each of the individual conjuncts are true as well.\\

\textbf{Biconditional Elimination:} $\frac{\alpha \iff \beta}{(\alpha \implies \beta) \land (\beta \implies \alpha)} \equiv \frac{\beta \iff \alpha}{(\alpha \implies \beta) \land (\beta \implies \alpha)}$\\

This allows us to convert if and only if statements into implication statements. Now, the next rule is important and will allow us to yield a sound and complete inference algorithm:\\

\textbf{Resolution:} $\frac{(x_1 \lor ... \lor x_{m-1} \lor x_m)\land(y_1 \lor ... \lor y_{m-1} \lor \lnot x_m)}{x_1 \lor ... \lor x_{m-1} \lor y_1 \lor ... \lor y_{m-1}}$\\

The resolution says that, if a clause contains a literal, and another contains it's negation, then both can be removed from it's respective clauses, and the disjunction of the clauses with literal and it's negation removed is known as the \textbf{resolvent}. This is true because, consider the conjunction of clauses $(P \lor x) \land (Q \lor \lnot x)$. For this sentence to hold, both clauses in the conjunction must be hold. But notice that only $x$ and it's negation $\lnot x$ can hold at once, and not both. If $x$ holds, then $\lnot x$ does not and hence $Q$ must hold, and vice versa. Hence $P \lor Q$ holds.\\

Alternatively, we can also think of it this: Given an existing clause with prior knowledge in the form of the literal $x$. Now, we have an additional inferred clause that says $\lnot x$ holds, which means $x$ is no longer true. Hence, we resolve the clauses by removing each of the literals $x$ and $\lnot x$. Hence, the resolution rule is \textit{sound}.\\

\subsubsection{Resolution Algorithm}

Remember that using propositional theorem solving, to show that $KB$ indeed entails $\alpha$, we want to show that $KB \implies \alpha$ is valid, or consequently that $KB \land \lnot \alpha$. The resolution proves this by contradiction as follows:

\begin{enumerate}
\item $KB \land \lnot \alpha$ is converted into conjunctive normal form(CNF). This is achieved using a combination of inference rules.
\item The resolution rule is applied to resulting clauses, which is the proof by contradiction process. Each pair of clauses that contains complementary literals is resolved to produce a new clause that is added back into the set. Then, one of 2 things will happen:
\begin{itemize}
\item No new clauses can be added in the set. That means that there exists a pair of generating clauses such that they do not contain complementary literals, and that they can potentially hold in some models, and hence $KB \implies \alpha$ is satisfiable. Therefore, $KB$ does not entail $\alpha$. 
\item 2 clauses resolve to produce the empty clause. Remember that literals are removed when they cannot both be satisfied, that is they are contradicting/complementary literals. Then, 2 clauses that resolve to become the empty clause, means that the generating clauses in $KB \implies \alpha$ can be reduced to clauses that cannot both hold at the same time. Hence,  $KB \implies \alpha$ is unsatistifable, and $KB$ entails $\alpha$.
\end{itemize}
\end{enumerate}

\begin{algorithm}
\caption{\textsc{PL-RESOLUTION}}\label{euclid}
\begin{algorithmic}[1]
\State \textbf{inputs:} $KB$ - knowledge base. $\alpha$ - propositional logic sentence
\\
\State $clauses \leftarrow $ set of clauses in CNF representation of $KB \land \lnot \alpha$
\State $new \leftarrow \{ \}$
\Repeat
\For{each pair of clauses $C_i, C_j$ in $clauses$} 
\State $resolvents \leftarrow \textsc{PL-RESOLVE}(C_i, C_j)$
\If{$resolvents = \emptyset$} \Return $true$
\EndIf
\State $new \leftarrow new \cup resolvents$
\EndFor
\If{$new \subset clauses$} \Return $false$
\EndIf
\State $clauses \leftarrow clauses \cup new$
\Until{all clauses resolved}
\end{algorithmic}
\end{algorithm}

We have seen that using the resolution rule yields inferences that are sound, because resolved rules are indeed entailed by $KB$ using propositional logic rules. Now we shall argue that it is complete:\\

\textbf{Completeness of Resolution}\\

First, let us introduce the concept of a \textbf{resolution closure}, $RC(S)$ of a set of clauses $S$, which is the set of all clauses derivable by repeated application of the resolution rule to the clauses in $S$, as well as their derivatives. We can see that for a finite number of symbols involved in the clause $P_1, ..., P_k$ that appear in $S$, we can only construct a finite number of resolved clauses. Additionally, we repeat the resolution process for each pair of clauses everytime we introduce new clauses into the set.Since we introduce only a finite number of resolved clauses into the set, then consequently the number of iterations of \textsc{PL-RESOLUTION} is also finite.\\

To complete the proof for completeness, we state the \textbf{ground resolution theorem:}\\

If a set of clauses is unsatisfiable, then the resolution closure of those clauses contains the empty clause.\\

We can prove this by it's contrapositive. Suppose that the resolution closure $RC(S)$ does not contain the empty clause. Then, remember that resolution is a form of reduction using propositional logic. That is, the resolved clauses are entailed by the generating clauses that were involved in the resolution to it. Since the generating clauses entailed a non-empty clause, we can find assignments for this resolved clause such that it holds. Hence, this means that  $KB \land \lnot \alpha$ is satisfiable.

\end{document}