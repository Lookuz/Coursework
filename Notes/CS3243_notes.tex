\documentclass[12pt]{article}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{microtype}
\usepackage[a4paper,margin=2cm]{geometry}
\usepackage{vwcol} 
\usepackage{lipsum,multicol}
\usepackage[colorlinks]{hyperref} 
\usepackage{caption}
\usepackage{pgfplots}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{tikz}
\usetikzlibrary{positioning}

% Expectation %
\setlength\parindent{0pt}
\DeclareMathOperator{\E}{\mathbb{E}}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}

\title{CS3243 Introduction to Artificial Intelligence}

\date{AY2019/20 Semester 2}
\author{Choong Wey Yeh}

\begin{document}

\maketitle

\section{Uninformed Search}

In this section, we will be discussing some of the basic uninformed search algorithms where these algorithms do not make use of certain heuristics to optimize their search strategy. Most of the time, we will observe that these basic uninformed search algorithms perform complete searches; that is they iterate over all the possible elements in the solution space before terminating.\\

For this section, we will be working with:
\begin{itemize}
\item \textbf{Fully-Observable Environment:} Each item in the solution space is known to the algorithm; i.e there are no hidden states
\item \textbf{Deterministic Setting:} No uncertainty/probabilistic notions involved in state change
\item \textbf{Discrete Environment:} States consist of discrete variables instead of continuous uncountable ones
\end{itemize}

Some of these examples include winning a sequence of moves in a single-player game, assembly of machine parts and boolean satisfiability(SAT) problem. Essentially, we can formulate each of these examples as a search problem where we iterate over the possible solutions. For example, in the SAT problem, we can perform a complete uninformed search over each combination of variables such that the boolean formula is satisfied. Of course, we will eventually see that this is not an efficient(nor practical) way to attain a solution for various problems.

\subsection{Problem Formulation}

Formally, we can define the basic elements of a problem definition using \textbf{states}, which are the current configurations for the problem(e.g the variable values for a SAT problem), and \textbf{actions} which causes a transition of one state to another via changing configurations(setting a boolean variable from true to false). When considering the configurations for state and actions, most of the factors are unnecessary and may be \textbf{abstracted out}. For the route planning problem in particular, we are concerned with how route the driver takes to get to a location, but \textit{not exactly how he drives there}, such as which lane or which car he uses. In this case, such information may be left out of the problem definition.\\

Additionally, we also have an \textbf{initial state}, which are the parameters that the algorithm starts with to search for the solution. This initial state can be default setting of certain parameters, or random initialization. But, we also need the following definitions:

\subsubsection{Goal State}

The \textit{goal state} is defined as the state that determines the termination of the algorithm as it has reached the objective of the solving the problem. When the search algorithm reaches a state that is deemed as the goal state, it will stop and return it's current solution. The goal state can be defined in various ways:

\begin{itemize}
\item \textbf{Explicit Set} of goal states, which can be the configurations that the algorithm permutates through. Often for problems with large solution spaces, this is not very practical as the set could be very large. 
\item \textbf{Implicit Function} $f(x)$ that evaluates the current configuration of the algorithm and outputs if the configuration is indeed a goal state or not, where $x$ is the current configuration used as the input.
\end{itemize}

Ideally, we want the evaluation of whether the current configuration is the goal state or not to be \textit{efficient}, as this evaluation function is called every single time the algorithm changes states, and could be called a large number of times(e.g if boolean formula in SAT is very long). Hence, an inefficient evaluation function could be computationally expensive and slow down the search process immensely.

\subsubsection{Path Cost}

When we are discussing about a search problem, very often each transition from one state to another incurs a certain cost or penalty, such in the route planning problem where there is a cost incurred moving from a location to another. Then, a sequence of state transitions. which are viewed as actions, is a \textit{search path} taken by the algorithm to find the optimal solution by viewing the possible actions that can be taken by the algorithm now as a decision tree like structure. We denote the cost of an action, such as a transition from one state to another as the following function:

\begin{equation*}
c(s, a, s')
\end{equation*}

where an action of $a$ causes the algorithm to transition from state $s$ to $s'$. Here, if $a_i$ is the $i^{th}$ action taken by the algorithm, then the sequence of actions $a_1, a_2, ..., a_n$ is the search path of the algorithm. If this sequence of actions results in the final state being the goal state, then it is a solution to the problem. Note that the solution \textbf{may not be unique}.

\subsubsection{Performance}

Not required for the problem formulation, but it is also eventually necessary to make sure that our algorithm works, and is efficient. In particular, we need to consider:

\begin{itemize}
\item \textbf{Termination:} We want for our solution to eventually find a solution. If the algorithm does not terminate at all, then we may never get an answer as the algorithm continues running
\item \textbf{Optimality:} We want a feasible solution, but above that we want to find the optimal feasible solution. That is, for a solution $y$, the solution is optimal if for all other feasible solutions $y'$, $c(y) \leq c(y')$. That is, the cost of our solution is minimal
\item \textbf{Efficiency:} defined as the search cost as well, is the time and space complexity of our algorithm with respect to the size of the solution space
\end{itemize}

\subsection{Search Strategies}

\subsubsection{Search Representation}

We shall represent the states and actions in our search problem as nodes and edges in a tree or graph, such as in \textbf{tree search} and \textbf{graph search}. Each node represents a state from the state space, and the edge connecting node $s$ and $s'$ represents a transition from state $s$ to $s'$.\\

Additionally, we also introduce the definition of a \textbf{frontier}. After we have reached a node $x$, the other nodes connected to $x$ are divided into those that are visited, and those that are not. The nodes that are visible from node $x$, but are not yet visited and can be expanded for exploration. These nodes form the frontier. Our search algorithm makes use of the frontier as a pool of nodes that are available to be visited next down it's search path. In tree search, we are allowed to repeat the nodes that we have visited(e.g backtracking), but in graph search we no longer return to a node once we have visited it.

\subsubsection{Search Performance}

Previously, we mentioned that the following criteria will be used to judge the performance of our algorithm:

\begin{itemize}
\item \textbf{Completeness:} Is the algorithm guaranteed to find a solution(if the solution exists)?
\item \textbf{Time Complexity:} How long does the algorithm take to find a solution in the worst case?
\item \textbf{Space Complexity:} How much space is required for the algorithm to perform the search?	
\item \textbf{Optimality:} Does the algorithm find the highest quality solution in the presence of multiple feasible solutions?
\end{itemize}

We shall explicitly evaluate the upcoming algorithms using the following parameters:

\begin{itemize}
\item \textbf{Branching Factor $b$:} The maximum number of children, or successors of a node in a search tree of graph
\item \textbf{Depth of Shallowest Goal Node $d$:} Height of the tree before the shallowest goal node is reached
\item \textbf{Maximum Depth $m$:} Maximum depth of the search tree
\end{itemize}

Instead of the standard time complexity evaluation using $V$ and $E$ as the number of nodes and edges, due to the large size of search problems.

\subsubsection{Breadth First Search}

The basic idea of BFS is that at any point of time in the search, the shallowest nodes are explored first. In particular, if the frontier consists of nodes from depth $d, d + 1,...$ then all nodes at depth $d$ are visited first before $d + 1$. BFS can easily be implemented by using a queuing function that accepts the queued neighbours of visited nodes, and outputs them in the order that it is received where newly generated states are polled last.\\

\begin{itemize}
\item \textbf{Completeness:} BFS is a very systematic strategy, where it observes all nodes of depth 1, then depth 2 and so on. Hence, at depth $d$, BFS would have finished visiting all nodes of depth $d-1$ and lesser. By the time BFS reaches depth $m + 1$, it would have visited all nodes of depth $m$ and before, which is all the nodes in the tree. Hence if a solution exists, it is sure to have found one.

\item \textbf{Time Complexity:} Consider the case where each node in the search tree has the maximum number of children $b$. Then, the first level of the search tree would generate $b$ children, and each of these children continues to generate another $b$ and so on. Then, the maximum number of nodes that the algorithm has to visit would be:
\begin{equation*}
1 + b + b^2 + ... + b^d = O(b^d)
\end{equation*}

where at the $i^{th}$ level, the search tree has $b^i$ children. Since the algorithm terminates when it has found the first goal node, if the shallowest goal node is at level $d$ then BFS would have eventually found it and does not continue past depth $d$.\\

While the time complexity is exponential in the input, BFS can be useful if heuristic says that the search tree is wide but shallow, or if the goal node is within a certain depth from the root.

\item \textbf{Space Complexity:} Similar to the time complexity, assuming the worst case scenario of a complete tree with branching factor $b$, then there are $O(b^d)$ children, and hence $O(b^d)$ amount of memory required to store all these children.

\item \textbf{Optimality:} If we consider a special case of search where the search step cost is 1, then the optimal solution for this search problem would be a feasible solution with the least number of nodes in the search path, which would be the shallowest goal node which has the lowest possible depth. Since BFS traverses the tree level by level, it would find the shallowest goal node first among all other goal nodes, which is the optimal solution.\\

However, most of the time the step cost is not unit, and in that case BFS does not usually find the optimal solution. This is because it finds the shallowest goal node, but it is not necessarily the \textit{least cost} path solution. We shall try to rectify this issue in the next search algorithm.
\end{itemize}

\subsubsection{Uniformed Cost Search}

UCS modifies the BFs strategy by always expanding the lowest cost node in the frontier, measured by a path cost function $g(n)$ as the cost of the path from the root to the current node $n$. It is trivial to see that BFS is UCS with path cost function $g(n) = depth(n)$. In implementation, we modify from BFS slightly again by changing the queue used a priority queue, or a heap data structure that emphasises cheapest paths.

\begin{itemize}
\item \textbf{Completeness:} UCS is complete, under the caveat that the minimum step cost is $\geq \epsilon$, where $\epsilon$ is a non-negative number. If there is no bound on the step cost, then we could have cases of negative costs or exponentially decreasing costs. In this case, the algorithm could be stuck without termination.

\item \textbf{Optimality:} Under the same conditions as completeness, the solution is optimal provided step costs are lower bounded by a non-negative number. This is because the first solution found by the algorithm is guaranteed to be the cheapest cost solution. If there were a cheapest cost solution, the algorithm would have found it first.\\

Note the observation that because step cost $\geq \epsilon$, the step cost must never decrease. In other words, $g(successor(n)) > g(n)$. However, if we lift the lower bound on step cost, then the search problem is reduced to that of BFS, because a long and expensive path may become the optimal by running into a node with extremely high negative cost. As a result, we need to search all possible paths to obtain the optimal.

\item \textbf{Time Complexity:} Remember that in the queue, we keep at most $b^i$ nodes in the queue if we are at depth $i$. Since the step cost $\geq \epsilon$, when the algorithm goes down a search path from depth $i$ to $i+1$, the overall current cost increases by at least $\epsilon$. Consequently, if the optimal path cost is $C^*$, then there are at most $\frac{C^*}{\epsilon} + 1$ nodes in the search path since the cost increases in sequence of $0, \epsilon, 2\epsilon,...,  \frac{C^*}{\epsilon}$. This also means the maximum depth the algorithm will go is at most $\frac{C^*}{\epsilon} + 1$, and hence the maximum number of nodes that it will visit is $b^{\frac{C^*}{\epsilon} + 1}$

\item \textbf{Space Complexity:} Similarly, since the maximum depth is $\frac{C^*}{\epsilon} + 1$, the maximum number of nodes that will be stored in the heap will be at most $b^{\frac{C^*}{\epsilon} + 1}$.
\end{itemize}

\subsubsection{Depth-First Search}

DFS expands one of the nodes the deepest level of the tree during the search instead of covering all nodes in a level. Only when the search hits the most bottom of the search path then does it go back to a shallower level and expand the nodes at those levels. This is good for search problems where it is beneficial to explore deeper rather than breadth. Implementation wise, DFS uses a stack instead of a queue to pick the next node.


\begin{itemize}
\item \textbf{Completeness:} Similar to BFS, DFS is complete as it eventually expands all nodes in a frontier and explore all possible paths, given that the depth of the search tree is \textit{finite}. For trees with infinite tree depths, then the algorithm may never terminate. 

\item \textbf{Optimality:} Like BFS, DFS is not optimal for the same reason that it does not use any heuristic on path cost at all.

\item \textbf{Time Complexity:} Assuming finite search tree height of $m$, let us consider the worst case that the optimal solution is a search path of length $m$ and that it is the rightmost path. Then, notice that because of the way DFS explores paths, DFS will exhaust all other possible paths before obtaining the optimal by depth-first traversal. If all nodes have the maximum successor branching of $b$, then that means DFS will go through all combinations of $b^m$ search paths in the worst case.

\item \textbf{Space Complexity:} DFS has a better space complexity than BFS of $O(bm)$. Each time the algorithm expands a node, it pushes all the successors of that node into the stack, which is at most $b$. Since the maximum depth, and the length of a search path is $m$, and DFS fully explores and exhaust a search path before it expands another node, the maximum number of items pushed on the stack is $bm$. \\

In fact we can continue to improve this space complexity further. Notice when we push the successors onto the stack, we pick one successor to continue exploring down. In this process, other successors are not used until all the possible paths following the picked successor is exhausted. In that case, we merely have to push the \textit{current} successor onto the stack to be tracked without the other successors. The algorithm can then backtrack and then pick another successor to expand along later in the search. Then, the space complexity is equivalent to the number of successors picked in a path, which is also equivalent to the maximum path length that is $O(m)$.
\end{itemize}

\subsubsection{Depth-Limited Search \& Iterative Deepening Search}

We have seen that for trees with infinitely long tree heights, then the algorithm could potentially run forever. If we knew that we only had limited time to run our search, or that the goal node is within a certain height, then we could limit the maximum height that DFS runs to. That is, we can force our algorithm to terminate once there are no longer any more search paths with length $l \leq l*$ where $l*$ is the maximum height specified.\\

However, note that if we do so, then our DLS algorithm is \textbf{not complete}. Because it only considers search paths of length $l^*$ and lesser, any search paths with longer lengths is ignored. If the goal node is of length $\geq l*$, then our algorithm would never return a solution.\\

To address this, we introduce an additional condition: To perform DLS \textit{until a solution is found}. This guarantees that the algorithm is complete and will return a solution to us. Additionally, this algorithm is similar to BFS in the sense that the search deepens level by level, like how BFS expands nodes in a depth first.

\begin{algorithm}
\caption{Iterative Deepening Search}\label{euclid}
\begin{algorithmic}[1]
\For{$depth = 0 \rightarrow \infty$}
	\State \textit{result} $\leftarrow$ DEPTH-LIMITED-SEARCH(\textit{problem, depth})
	\If{$result \neq $ cutoff}
		\State \textbf{return} \textit{result}
	\EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

This allows us to maintain the space complexity benefit of DFS over BFS, but also allows us to address the problem of large state space and unknown tree depth(which is often most of the time).\\

However, now we have another downside: we essentially repeat searches over subtrees of the search tree. We first perform DLS with depth $d$, then $d + 1$ and so on. Notice that the search space on the search tree limited with depth $d$, is a subspace of the search space on a tree with depth $d + k$. Hence, we are repeating some of the search from the previous iteration of IDS. In particular, the number of nodes expanded by DLS on depth $d$ is:

\begin{equation*}
1 + b + b^2 + ... + b^{d-1} + b^d
\end{equation*}

and the number of nodes expanded by IDS is:

\begin{equation*}
(d+1)1 + (d)b + (d-1)b^2 + ... + 2b^{d-1} + 1b^d
\end{equation*}

Notice that of all the subtrees of different depths of the search tree, IDS explores subtrees of shallowest heights the most, while the bottom-most parts of the trees is explored lesser. This is because of the fact that in every iteration of IDS, it performs DLS restarting from the root again. Hence the initial parts of the search tree are traversed more often. \\

In particular, the further down the tree the search goes, the number of successors increases, perhaps exponentially. Since these parts of the tree are explored much lesser than the top, which has much lesser nodes, in perspective the search is repeated on a very small subset of the nodes of the entire search space considered, and hence the overhead in practice is actually within accepted bounds.

\paragraph{Remarks}
\begin{itemize}
\item If $b$ is not finite, then all the uninformed algorithms we have discussed are not complete, since they will never be able to exhaust fully nodes of any depth
\item If $b$ is finite, then BFS and IDS is complete, since IDS iteratively increases the depth similar to BFS. Additionally, if the step cost is lower bounded by $\epsilon$, then UCS is also complete
\item If $m$ is finite, then DFS is complete, otherwise it may continually run forever down an infinite search path
\item if the step cost is unit(i.e = 1), then UCS is reduced to BFS, and BFS and IDS are optimal because the step cost is equivalent to the number of nodes in search path
\end{itemize}
\pagebreak

\section{Informed Search}

Previously, we have seen search strategies that makes use of the problem definition, such as the states and actions, to perform a general search action. This usually results in some form of queuing function that determines which node to expand next deterministically. However, more often than not we do not have enough information to full model the problem to determine the full cost. For example,

\begin{itemize}
\item Depth of search tree or size of solution space could be possibly infinite
\item Repeated states could be present
\item Certain constraints could be hard to model, or cause problems to be difficult to solve
\end{itemize}

If we use uninformed search strategies for these problems, optimality and completeness may not be guaranteed, in addition to the exponential time search efficiency. In this case, we can introduce additional information or heuristics that can be exploited in order to improve our search strategy. To do this, we introduce an evaluation function $f(n): X \rightarrow R$ which returns a number indicating the desirability of the node to expand next. This could be in terms of shortest path cost for example. The evaluation function that we define here serves to approximate the cost estimate from node to node in problems where full information is not present for us to perform uninformed search.

\subsection{Best-First Search}

In best-first search, we make use of the evaluation function by expanding the node that has the most desirable evaluation function score, by ordering the nodes by their evaluation function values. If the evaluation function is able to perfectly reflect the information stated in the problem definition, that is if the evaluation function is a true one, then expanding the node by the evaluation function score will be the optimal path and indeed be the best node to expand at the current point in time. However, this is often not the case and the evaluation function will reflect the node that \textit{appears} to be the best node to expand in that point in time.\\

\begin{algorithm}
\caption{Best-First Search}\label{euclid}
\begin{algorithmic}[1]
\State $result \leftarrow$ next node ordered by $f(n)$
\If{$result == goal\ state$}
	\State return $search\ path$
\Else
	\State goto $1$
\EndIf
\end{algorithmic}
\end{algorithm}

\subsubsection{Greedy Best-First Search}

A good Best-First Search strategy is to \textit{minimize estimated cost to reach the goal}. That is, we define a \textbf{heuristic function} $h(n)$ that estimates the cost from a node to the goal node. Often this function provides not an exact formulation of the actual cost from a node to the goal node, but a good enough estimate of this cost. Then, we set:

\begin{itemize}
\item $f(n) = h(n)$ and use the heuristic function to expand the next node
\item $h(t) = 0$ where $t$ is the goal node, as the distance from the goal node to itself is clearly 0
\end{itemize}

\subsubsection{Performance Measure}

\begin{itemize}
\item \textbf{Completeness:} Similar to the uninformed search functions, Best-First Search also makes use of a queuing function, albeit using an evaluation function. This means that every node will eventually be added into the queue, and expanded until the solution is found if $b$ is finite. 

\item \textbf{Optimality:} However, it is easy to see that Best-First Search \textbf{is not optimal}. Notice that Best-First Search aims to obtain low cost solutions by using the estimated cost from the node in the frontier to the goal node. However, \textit{it does not make use of past information of the cost of the path travelled so far}. Best-First Search focuses on greedily reducing the current estimated remaining cost to the goal, and completely ignores the overall path cost to reach the goal node.\\

In order to focus the search on lowest cost paths toward the goal, our evaluation function must incorporate the \textit{cost of the path} from a state to the goal state, instead of ignoring the path of the cost so far to the current node.

\item \textbf{Time Complexity:} In the worst case where the path containing the least cost weights has length of that of the height of the search tree, then the search will incur runtime exponential in the order of the branching factor $O(b^m)$

\item \textbf{Space Complexity:} Similar to the time complexity, in the worst case we may maintain $b^m$ elements in the queue for a search tree with branching factor of $b$ and a search tree depth of $m$.
\end{itemize}

Apart from in-optimality issues due to ignoring of overall path cost, Best-First Search also has various other issues:

\begin{itemize}
\item \textbf{Susceptible to False Starts:} Because the search strategy only minimizes the current remaining estimated cost, it could be possible for the algorithm to pick the shortest node that leads to a dead end, resulting in unnecessary nodes being expanded.
\item \textbf{Repeated States:} Furthermore, if we allow repeated states such as in Graph Search, then it could be possible for the algorithm to oscillate between 2 visited states if the estimated cost between the two is the cheapest among those in the frontier.
\item \textbf{Infinite Search Path Length:} Similar to Depth-First Search, if the search tree path has infinite length, then it is possible for the algorithm to keep searching down that path and never terminate
\end{itemize}

\subsection{A* Search}

We have seen that minimizes the estimated cost to the goal using the heuristic function $h(n)$ in a bout to reduce search cost, but it is not optimal. On the other hand, Uniformed-Cost Search(UCS) reduces the cost of the path so far to the goal, but it is inefficient due to it's exponential time complexity. But we can combine both strategies to obtain a search strategy with the advantages of both. We do this by summing up the the evaluation functions of both search algorithms where:

\begin{itemize}
\item $g(n)$ is the cost function from UCS that measures the cost from the start node to the current node $n$
\item $h(n)$ is the cost estimate heuristic from Best-First Search that gives an estimate of the remaining cost from the node $n$ to the goal node
\item $f(n) = g(n) + h(n)$ is the combined heuristic function that estimates the cost of the cheapest path from the start node to the goal node by expanding node $n$ from the current node.
\end{itemize}

Hence if we are aiming to minimize the overall cost of the search path, then minimizing $f(n)$ is a reasonable strategy. In fact, we will soon prove that given the following restriction on $h(n)$, then the search strategy is complete and optimal:

\begin{theorem}
If $h(n)$ is an \textbf{admissible heuristic}, then A* Search using Tree Search is optimal
\end{theorem}

\begin{definition}
A heuristic function $h(n)$ is an \textbf{admissible heuristic}, if $\forall n, h(n) \leq h^*(n)$ where $h^*(n)$ is the optimal true cost to reach the goal state from node $n$. 
\end{definition}

That is, the algorithm \textit{never overestimates the true cost of the path from node $n$ to the goal}.\\

Furthermore, we shall assume the \textbf{monotonic} property for the path costs for A* search that, all path costs are non-decreasing. That is for the path containing the node $n$ to the goal node $t*$, $f(n) \leq f(t*)$. In particular we say that the heuristic function is \textbf{consistent}.

\begin{definition}
A heuristic function $h(n)$ is \textbf{consistent}, if $\forall n$, and it's successors $n'$, $h(n) \leq c(n, n') + h(n')$.
\end{definition}

In some cases, we also call this heuristic function metric because it obeys the triangle inequality. A result from this definition is that path costs are non-decreasing, or monotonic. This can be seen via the following equation:

\begin{equation*}
\begin{aligned}
f(n') & = g(n') + h(n') \\
& = g(n) + c(n, n') + h(n') \\
& \geq g(n) + h(n) = f(n)
\end{aligned}
\end{equation*}

In particular, the following lemma holds:

\begin{lemma}
If a heuristic is consistent, then it is also admissible
\end{lemma}

A simple proof of this is that, we can take any node $n$, then we see that:

\begin{equation*}
\begin{aligned}
h(n) & \leq c(n, n') + h(n')\\
& \leq c(n, n') + c(n', n'') + h(n'') \\
& ...\\
& \leq c(n, n') + c(n', n'') + ... + c(n^{t-1}, t) + h(t) \\
& \leq c(n, n') + c(n', n'') + ... + c(n^{t-1}, t) (\text{remember $h(t) = 0$})\\
& \leq c(n, t)
\end{aligned}
\end{equation*}

If the path costs for a problem is non-monotonic, then we can use the following function to enforce monotonicity using the following $pathmax$ function:

\begin{equation*}
f(n') = max(f(n), g(n') + h(n'))
\end{equation*}

By the monotonicity property of the path costs, that means that A* will expand all nodes $n$ with evaluation function costs $f(n) < f*$ where $f*$ is the cost of the optimal solution path. This is similar to the BFS behaviour where A* expands nodes layer by layer, where each layer is a contour represented by it's evaluation function cost.

\subsubsection{Proof of Optimality of A* Search}

Let the optimal goal state in the search tree be $t*$, and a suboptimal goal state be $t$. Then, $f(t) = g(t) \leq f(t*) = g(t*)$. Let us then assume then A* algorithm has picked $t$ as the solution and terminated. We shall see that this is not possible.\\

Let $n$ be an unexpanded node on the path to the optimal goal state $t*$. Such a node exists because if it does not, that means that the algorithm that already expanded all nodes on the path to $t*$, and hence found the optimal goal state. Additionally, let it be such that 

\begin{equation*}
f(t) \leq f(n)
\end{equation*}

such that $t$ is expanded before $n$, which can be rewritten as:

 \begin{equation*}
 \begin{aligned}
f(t) \leq f(n) & = g(t) + h(t) \leq g(n) + h(n)\\
&= g(t) \leq g(n) + h(n)
\end{aligned}
\end{equation*}

since $h(t) = 0$ as $t$ is a goal state. Then, because path costs are non-decreasing, that means that:

 \begin{equation*}
 \begin{aligned}
g(t) & \leq g(n) + h(n) \\
& = f(n) \\
& \leq f(t*) \\
& = g(t*) + h(t*) = g(t*) (\text{since $h(t*) = 0$})
\end{aligned}
\end{equation*}

Which means that the path cost of reaching the suboptimal goal state $t$ is lesser than that of the optimal goal state $t*$, which contradicts our initial assumption that $f(t) = g(t) \leq f(t*) = g(t*)$. Hence, we have proven that A* search is guaranteed to find the optimal goal state. Intuitively, we can see that this is true because $f(t) \leq f(t*) = g(t*) = g(n) + h*(n)  \leq f(n) = g(n) + h(n)$ for any node $n$ that is on the optimal path from the start node to the goal node due to non-decreasing costs, and by the admissible heuristic that $h(n) \leq h*(n)$. Moreover, it is also easy to see that if we have a heuristic that \textit{overestimates} the path cost to the goal node, then the algorithm might evaluate $f(n) > f(t)$ and eventually find the suboptimal goal first.\\

Additionally,  a stronger claim is that when a node $n$ is selected for expanding, that means that \textit{the path leading up to $n$ is the shortest path found}. To see why this is so, we shall assume that the algorithm expands node $n$ via a suboptimal path with last node $q_2$ instead of a optimal path with last node $q_1$. We know that since $n$ was expanded before $q_1$, that means that $f(q_2) \leq f(n) \leq f(q_1)$. But this contradicts the optimality of the optimal path with cost $f(q_1) < f(q_2)$, and the non-decreasing property since $q_1$ is on the path to $n$ and hence $f(q_1) < f(n)$.\\

In particular, it is also proven that A* is \textbf{optimally efficient}. That is, any other search algorithm that uses an admissible heuristic that finds the optimal goal state \textit{does not expand less nodes} than A*. This also means any algorithm that does not expand nodes $n$ with $f(n) 
\leq f(t*)$ run the risk of non-completeness and not finding the optimal goal state.

\subsubsection{Proof of Completeness of A* Search}

Recall that we mentioned that path costs are non-decreasing along the path. We can visualize the search landscape of the search tree in contours, where each contour is in order of increasing path costs. We can see that A* Search eventually expands all nodes in a contour before moving onto the next. In particularly, A* search moves in increasing order of contours using the evaluation function. This means that A* search will expand up the contours, before it eventually reaches the optimal shallowest goal node.\\

However, this is only true under 2 conditions:
\begin{itemize}
\item If there are finite number of nodes with evaluation function costs $f(n) < f*$
\item If the branching factor $b$ in the search tree is finite
\end{itemize}

Similar to problems with uninformed search, we can see that if the above 2 conditions do not hold, it is possible for the algorithm to be stuck within a contour and never terminate.

\subsubsection{Time and Space Complexity of A* Search}

While A* Search is optimally efficient, it still incurs time complexity exponential in the base $b$, to the power of the error incurred in the estimation of the heuristic function resulting in $O(b^{h^*(n) - h(n)})$, unless the error grows no faster than the logarithm of the actual path cost(proof omitted):

\begin{equation*}
|h^*(n) - h(n)| \leq O(\log h^*(n))
\end{equation*}

In particular, we wish to use dominant heuristic functions over a set of possible $h(n)$

\begin{definition}
For 2 heuristic functions $h_1(n), h_2(n)$ that are admissible, if $\forall n, h_1(n) \geq h_2(n)$, then $h_1(n)\ \textbf{dominates}\ h_2(n)$
\end{definition}

This also means that $h_1(n)$ incurs lower search costs, because since both functions are admissible, values for $h_1(n)$ are closer to the true cost function $h^*(n)$ than $h_2(n)$ is.\\

This means that A* is greatly affected by the error incurred by the heuristic function estimation. However, computational time is not the biggest drawback. A* incurs a space complexity of $O(b^m)$ by maintaining nodes generated in the frontier in memory. More often than not, A* Search will run out of space before it finishes running.

\subsection{Deriving Admissible Heuristics}

Often we have to invent our own heuristic functions in order to tackle a search problem. One of the ways that we can do this is by \textbf{relaxing} the constraints to the problem. This means removing some of the restrictions placed on the problem itself, leading to special cases of the problem that can be solved using specific derivable heuristics. It is often that \textit{optimal solutions to a relaxed problem is often a good heuristic to solving the original}. By relaxing a problem, we can develop a set of admissible heuristic functions $\{h_1, h_2,..., h_n\}$ that provides optimal or near optimal solutions to specific configurations to the search problem, which can be used to evaluate different scenarios.\\

With a set of specific heuristic functions, it follows that each of these heuristics that developed to solve a specific scenario, and not a general best heuristic for the best solution itself. If we pick one heuristic over another, it could be that the heuristic does optimally in one scenario, but very bad in another. Hence instead of picking, we can make use of the dominant property to pick heuristic functions at the current state:

\begin{equation*}
h(n) = \max (h_1, h_2, ..., h_n)
\end{equation*}

Which means that always have the best option of a heuristic at any state in the search. Using the dominant property, as well as the fact that the heuristics are admissible, guarantees the best choice of heuristics among all we have by picking the heuristic closest to the upper bound of the true cost.\\

Other methods of derivation of heuristics involve statistical learning and feature evaluation as well, which allows the algorithm to derive importance of various features of heuristics in different situations by itself.

\pagebreak
\section{Local Search}

In some problems, the state description contains all the information about the problem that we need to make informed searches, and the path in which we take during our search may not be so useful anymore. We only require that the goal state be reached. In this case, local search, or \textbf{iterative improvement} algorithms may provide a practical approach that does not require searching over search paths. More often than not, it is often a good idea to \textit{start with a complete configuration of the problem, and make gradual improvements to the solution}. We can easily see that such algorithms search a smaller portion of the search space while still being able to provide reasonable solutions to the problem. Local search algorithms do not look far into a search path, and usually only considers neighbours within close proximity.

\subsection{Hill-Climbing Search}

The hill-climbing algorithm is one that continually moves the current position in the direction of an increasing objective value. The algorithm always aims to make an improvement through a movement to it's surrounding neighbours if possible. Variants of this algorithm in other problems such as convex optimization may also be known as  gradient descent.\\

\begin{algorithm}
\caption{Hill-Climbing Search}\label{euclid}
\begin{algorithmic}[1]
\State $current \leftarrow problem.$initial-state
\Loop
	\State $neighbour \leftarrow$ highest-valued successor of $current$
	\If{neighbour.Value $\leq$ current.Value}
		\State \Return current.State
	\Else 
		\State $current \leftarrow neighbour$
	\EndIf
\EndLoop
\end{algorithmic}
\end{algorithm}

Note that unlike the previous algorithms that we have seen, the Hill-Climbing search algorithm does not maintain any form of search tree or graph, but only the current state, it's neighbour states and their evaluations. This cuts down the need for large memory requirements immediately.\\

However, the Hill-Climbing search paradigm does suffer from certain drawbacks:

\begin{itemize}
\item \textbf{Local Maxima:} The algorithm may run into a point where the current state is a peak among all other neighbours, and returns this state. That is, the algorithm is guaranteed to return a \textit{maxima}, but it could be a local maxima and be maximal with respect to the neighbours, but not the entire state space and be a \textit{global maxima}. The algorithm does not know this and only recognises the local maxima as the highest value, since it does not look past the neighbours of the current state. In many cases, the local maxima returned by the algorithm may be far from satisfactory compared to the global maxima.

\item \textbf{Plateau:} This is an area of the state space where the value does not change from current to neighbouring states. In this case, the algorithm does not know for sure which move to take, and either uses heuristics to decide or makes a random walk.
\end{itemize}

The drawbacks that Hill-Climbing search has points to the fact that once the algorithm is unable to make any further improvement, it returns the current best solution without checking other peaks in the state space, and most of the time the current highest returned is a local maxima and not the optimal solution to the problem. That is, Hill-Climbing fully uses exploitation, but not exploration in it's search. We can combat this in various ways depending on the problem, but the most commonly known is \textbf{Random-Restart Hill-Climbing} by restarting the algorithm initialized to different starting states in a bout to obtain a higher maxima.

\section{Adversarial Search}

Unlike previous search problems that were defined before, we now look at problems where \textit{more than one player} is involved. For example, chess can be modelled as a search problem playing against another player and trying to make the best moves at each point in time. These problems are also called \textit{games}, and model real world problems much more accurately than previous problems.\\

Unfortunately, the presence of additional agents also introduce multiple factors that increase the intractability of solving the problem:

\begin{itemize}
\item \textbf{Uncertainty} of the next environment states in the game, since the states are no longer influenced just by the agent's actions, but also other agent's actions as well.

\item \textbf{Larger search space} due to the increase in the total number of combinations of moves resulting from additional agents participating in the game. The agent needs to account for the possible moves that the opponent might take in chess, for example.
\end{itemize}

\subsection{Two-Person Games}

Let us being by considering games that involves 2 agents or players. In particular, we wish to look at \textbf{zero-sum games}, which are games where the gain or loss by a participant is exactly balanced and reflected in the other player's utility. For example, in a gambling problem involving a banker and the player, if the banker has a utility of $x$, that means that he/she has won $x$ dollars and consequently the player has lost $x$ dollars and has a utility of $-x$. Additionally, we reformulate the following:

\begin{itemize}
\item \textbf{Initial state} includes the starting board position, and who goes first
\item \textbf{Set of operators} determines the legal moves that the current play can make
\item \textbf{Terminal test} defines whether or not the game is over depending on win conditions, or if no more legal moves can be made
\item \textbf{Utility/Payoff Function} returns a numerical value on the outcome of the game. In the game of chess, a win for the MAX player would give +1, a win for MIN player would give -1 and a draw would be 0
\end{itemize}

We also see that for zero-sum games, we define a MAX and MIN player, that aims to maximise and minimize the utility function respectively. We can see that the 2 agents now have objectives that contradicts each other, and makes the game harder for the opponent. More importantly, any gain or loss for the MAX player is a loss or gain for the MIN player respectively, and we can see that the total utility \textit{sums to 0}, which reflects the definition of a zero sum game.\\

\subsubsection{Minimax Algorithm}

Let us consider the search tree for formulating a 2 player zero-sum game. Note that the plays alternate between the MAX and the MIN player, and hence each depth of the search tree alternates between player MAX and MIN. The minimax algorithm uses this search tree to formulate the optimal moves for the MAX player that starts the game first, by deciding the best optimal starting move.

\begin{enumerate}
\item The algorithm first generates the entire search tree for the game, all the way to the terminate states that are represented by the leaves in the search tree. Each depth of the tree alternates between MAX and MIN's moves, with the root of the tree representing the starting move by MAX

\item From the terminal states at the leaves, we apply the utility function to each leaf to determine the the utility value. This represents a +1 for a win for MAX, and -1 for a win by MIN for a game of chess for example.

\item We recursively move one level up the tree everytime the utility values of the current depth is determined for all nodes, and use the utility value for the current depth to decide the utility value for the nodes one level up. In particular, let $S_i$ be the set of children residing in depth $i$ of node $v_{i+1}$, and $f$ the utility function used in the game. If level $i+1$ denotes a play by MAX, then 

\begin{equation*}
f(v_{i+1}) = \max_{u \in S_i} f(u) \text{ if player = MAX}
\end{equation*}

That is, at level $i+1$, MAX will pick the move out of all possible children that will lead it to the terminal state that yields the highest possible utility. Similarly for MIN, if the current level corresponds to a play by MIN, then 

\begin{equation*}
f(v_{i+1}) = \min{u \in S_i} f(u) \text{ if player = MIN}
\end{equation*}

instead to minimize the overall utility.\\

In particular, we see that everytime the levels alternate, the utility switches from maximizing to minimizing based on the MAX and MIN player. This is because the 2 objectives of the players are clashing, and everytime MAX makes a move that maximizes the utility, in the next half-move by MIN player, he/she will counter MAX's progress by choosing a path that minimizes the overall utility. As such, MAX and MIN never really often attain the maximal/minimal utility, but rather, the \textit{maximum minimal utility} for MAX, and vice versa for MIN. Hence the name, Minimax algorithm.
\end{enumerate}

In particular, at each level of the search tree, the move taken by the player to maximize/minimize the utility is known as a \textbf{strategy}.  

\begin{definition}
A strategy $s_1$ for player 1 is called \textbf{winning} if for any strategy $S-2$ by player 2, the game ends with player 1 as the winner.\\
A strategy $s_1$ for player 1 is called \textbf{non-losing} if for any strategy $s_2$ by player 2, the game ends in either a tie or a win for player 1.
\end{definition}

Another important observation is that at each level, the player that makes the play at that depth plays the best that they can to possibly maximize/minimize the utility, and we cannot change the search path choice to improve the utility. The minimax algorithm is also known to output a \textbf{sub-perfect Nash equilibrium}. While the choice over the entire tree may not be optimal, but at each play, the player makes the best choice they possibly can.

\begin{itemize}
\item \textbf{Completeness:} If the maximum depth of the tree $m$ is finite, and the number of legal moves(which translates to the number of branches $b$) is finite, then the algorithm is able to fully generate the tree and is complete.

\item \textbf{Optimality:} Similar to completeness, since $m$ and $b$ are finite, the entire search tree with it's respective utility costs is visible to the algorithm. That means that the algorithm has perfect information to the utility cost of each move, and is able to make the best move that maximizes/minimizes it's utility. Hence, it would be able to output the optimal solution.

\item \textbf{Time Complexity:} For a search tree of branching factor $b$ and depth $m$, in the worst case it needs to generate a tree with $b^m$ nodes, and hence takes $O(b^m)$ time to do so

\item \textbf{Space Complexity:} Because of the recursive nature of generating the nodes, Minimax algorithm maintains nodes in a search path by diving into the terminal states before going back up, using at most $O(bm)$ space.
\end{itemize}

\subsection{Alpha-Beta Pruning}

We have seen that for the minimax algorithm to output the optimal solution, it needs to generate the entire search tree which hinges on it being finite. However, even for finite trees, it can be seen that sometimes search trees can be very large, and it may take a long time for the algorithm to perform search.\\

Fortunately, it is possible for Minimax to be optimized to search and output the optimal solution despite not looking at every single node in the search tree, using the intuition that \textit{we do not explore nodes that are already bad}. Consider a node $n$ along the search tree. If the player has a better choice at node $m$ which is a parent of, or higher up the search tree from $n$ that improves his/her utility, then node $n$ will never be reached. Hence, it is not necessary to explore the paths further down from $n$.\\

Since Minimax is depth first, we only need to consider nodes along a single path in the tree. We denote $\alpha(n)$ as the highest observed value found on any path from $n$ initialized at $- \infty$, and $\beta(n)$ as the lowest observed value initialized at $\infty$. Intuitively, this can be seen as $\alpha$ is the maximal possible utility for MAX, and $\beta$ the minimal for MIN, and any further values of $\beta$ that do not improve the value of the current $\alpha$ will be pruned. In particular,

\begin{itemize}
\item given a node $n$ at a level corresponding to a MIN play, we stop searching below $n$ if there is a MAX ancestor $i$ of $n$ such that $\alpha(i) > \beta(n)$
\item given a node $n$ at a level corresponding to a MAX play, we stop searching below $n$ if there is a MIN ancestor $i$ of $n$ such that $\beta(i) < \alpha(n)$
\end{itemize} 

Note that alpha-beta pruning prunes aways branches of the search tree does will never get reached, and hence these branches do not influence the final decision. But how much computational effort does alpha-beta pruning actually save? This depends on the ordering of the moves that are legal, which determines the search path ordering. With perfect ordering, alpha-beta pruning can give a time complexity on Minimax of $O(b^\frac{m}{2})$. This means that the algorithm can actually search twice as deep with the same time constraints. However, even without a good ordering, on average alpha-beta pruning gives a time complexity of $O(b^\frac{3}{4})$ in a random ordering.

\subsection{Evaluation Functions}

When search gets large, we also have the option of limiting search, just as in DLS. However, for cases like Minimax algorithm, we run into several major problems, the first being that since depth is limited, we may not reach the terminal states for certain search paths. Because the utility of intermediate noes rely on the terminal states to determine the actual utility, that means that we are unable to determine intermediate nodes of search paths that do not reach the terminal state.\\

We rectify this by introducing an \textbf{evaluation function}, which is an approximation function that evaluate how good an intermediate state is. We require that the evaluation function satisfy the following conditions:

\begin{itemize}
\item The evaluation function must agree with the utility function on terminal states
\item The evaluation function cannot be too expensive, in particular it cannot be more expensive than the utility function as an approximation. 
\item The evaluation function must be accurate in reflecting the player's odds of winning
\end{itemize}

Additionally, we also need to ensure that we do not run into the \textbf{horizon problem} when we perform the search limited. If we cut off search at a state that is volatile, and is threatened by possible wild swings or moves that may turn the game around, then it could be possible that the winning odds reflected at the state before and after the cutoff reflect very different information. Hence, we also include the condition that the search be performed til a \textbf{quiescent} position before being cut off, which is one that is robust to large changes.

\subsection{Expectimax Algorithm}

In some games that involve stochastic layers, we also need to incorporate randomness into the game itself such as Poker. In addition to our standard minimax algorithm search tree, we also need to insert 'chance' nodes that represent as random choice between state changes. For each of the children of chance node $i$, $x_j$, we represent the probability that the search goes down to node $x_j$ is denoted $p(x_j)$. At the terminal nodes, we continue to use our utility function to obtain the utility value. However when we propagate upwards to the chance nodes, then we are unsure of exactly which path we will take. Instead, we take the $expected$ utility value upon reaching that node, which is the combination of the utility values among all children weighted by the probabilities of hitting that node.

\begin{equation*}
\begin{aligned}
f(x_i) = E[f(x_i)]  = \sum_{j \in J} p(x_j) f(x_j)
\end{aligned}
\end{equation*}

By introducing chance nodes and randomness into our search algorithm, we have increases the time complexity from $O(b^m)$ to that of $O(b^mn^m)$ because of the fact that we could have up to $n$ chance nodes at each of the $m$ levels.

\pagebreak
\section{Constraint Satisfaction Problems}

In some problems, in addition to the basic requirements of obtaining the optimal solution or reaching the goal state, we also need to satisfy additional requirements or properties for the solution to be feasible. These properties are called \textbf{constraints}, and the problem can be modelled as what is called a \textbf{Constraint Satisfaction Problem(CSP)}. In CSPs, the states are represented in the form of variables, and constraints that the goal state must obey. For example:

\begin{equation*}
\begin{aligned}
&\min c^T x\\
&s.t \textbf{	} Ax = b\\
&x \geq 0
\end{aligned}
\end{equation*}

is commonly seen as the standard form of the Linear Programming problem. \\

Each variable $x_i$  has a domain $D_i$ that it takes it's values from, such as $D_i = \mathbb{R}$ for real numbers, or $D_i = \{0, 1\}$ for binary values.This domain can be discrete or continuous, but CSP under continuous domains falls under the previously mentioned Linear Programming problem, and for this section we focus on discrete values. Additionally, constraints can be the following:

\begin{itemize}
\item \textbf{Unary Constraints} involving one variable: $x_i = a$
\item \textbf{Binary Constraints} involving 2 variables: $x_i + x_j \leq a$
\item \textbf{Higher Order Constraints} involving multiple variables
\end{itemize}

Note that in this case, we are using integer operations on constraints. This is defined by our \textbf{Constraint Language}, which represents \textit{how} our constraints should be formulated.\\

Converting our CSPs back to the form of search state spaces, we can denote our initial state as an empty assignment; that is no values are assigned to any variables, and the transition functions being the assigning of values to variables. Lastly, our goal state is an assignment for the variables such that the solution is feasible. In particular, we say that the assignment is $complete$ if the assignment does not violate any constraints in the CSP, and $\forall i, y_i \in D_i$ .\\

A standard search paradigm for working with CSPs is as follows:

\begin{algorithm}
\caption{Standard CSP Search}\label{euclid}
\begin{algorithmic}[1]
\State $assignment \leftarrow []$
\While{$assignment$ is not complete}
\State $assignment \leftarrow assignment \cup {y_i}$ where $x_i = y_i, y_i \in D_i$
\State \If{$assignment$ is complete} 
	\Return $assignment$
\EndIf
\EndWhile
\end{algorithmic}
\end{algorithm}

Note that there are a few quirks of simply following the above routine:
\begin{itemize}
\item \textbf{Repeated States} but different order of assignments. This means that in term of a search tree, then we would have many unnecessary nodes that represent the same state(e.g assigned $x_i = a$ then $y_i = b$ in one path and the reverse order in another path. Notice order is not required. We can merely represent each level of the search tree as the assignments for a $single variable$. Then, for $d$ dimensional variable and $m$ variables, the search tree is at most $O(d^m)$. This is also known as \textbf{Backtracking Search}, where we assign a variable per level.

\item \textbf{Running into already invalid assignment} but still continuing to search. In many cases, we can already determine if progress from the current state will lead to a valid assignment. If the current state can no longer reach a valid assignment, we can simply stop the search down that sub tree and effectively prune search in that subtree.
\end{itemize}

\subsection{Backtracking Search}
\begin{algorithm}
\caption{Backtracking Search}\label{euclid}
\begin{algorithmic}[1]
\State $assignment \leftarrow []$
\While{$assignment$ is not complete}
\State \If{$assignment$ is complete} 
	\Return $assignment$
\EndIf

\State $var \leftarrow$ Select-Unassigned-Variable$(csp)$
\For{each value in Order-Domain-Values$(var, assignment, csp)$}
\If{$y_i$ is consistent with $assignment$}
\State add $\{ x_i = y_i\}$ to $assignment$
\State $inferences \leftarrow$ Inference$(csp, var, value)$
\If{$inferences \neq failure$}
\State add $inferences$ to $assignment$
\State $results \leftarrow$ Backtracking Search($assignment, csp$)
\If{$result \neq failure$}
\Return $result$
\EndIf
\EndIf
\EndIf
\EndFor
\EndWhile
\end{algorithmic}
\end{algorithm}

Notice that 
\begin{itemize}
\item \textbf{Order of variable assignment is irrelevant}, similar to how we do not bother about the search path and concentrate on finding a valid assignment
\item \textbf{At every level, consider a single variable}. This considerably reduces our search space for assignments
\item \textbf{Use of inferences}, which tells us what we can say about the remaining values given the current assigned values. That is, the currently assigned values determine which of the remaining values are valid or not.
\end{itemize}

In particular, we can improve efficiency and save runtime by optimizing the subroutines: \textbf{Select-Unassigned Variables, Order-Domain-Values and Inferences}. Basically, \textit{how do we choose what to assign each variable at each point in time}. The following as heuristics we can use:

\begin{itemize}
\item \textbf{Minimum Remaining Values(MRV):} Pick the variable with the \textit{fewest} legal values left. This allows us to detect  invalid assignments quicker before searching too deep and wasting search time.
\item \textbf{Degree(Most Constraining Variable) Heuristic:} Pick the variable that has the most relations with other variables. That is, it \textit{influences} other variables the most. Similar to MRV, selecting this variable first allows us to quick eliminate assignments to other variables made invalid because of this variable quickly.
\item \textbf{Least Constraining Value} for \underline{value selection} to be assigned to the variable. We want values assigned to variables to be permissive, so that we are more likely to find a valid assignment and not waste time searching only to reach an invalid solution.
\end{itemize}

\subsection{Inferences}

\subsubsection{Forward Checking}

In forward checking, the main idea is to keep track of remaining illegal values for unassigned variables. At any point in the time in the search, if any of the variables has no remaining legal values left, then it ay progress cannot output a valid assignment since one variable cannot be satisfied. Hence, we terminate search down the current subtree and return back to a state where a valid assignment is still possible.

\subsubsection{Constraint Propagation}

Using forward checking, we are able to terminate search down a subtree that is guaranteed to not have a valid assignment, and hence prunes portions of the search tree to reduce search space. But we can improve this further. Note that forward checking does not detect early signs of invalid assignments. This is where \textbf{constraint propagation} comes in. Using constraint propagation, we are able to observe if values are illegal by reducing and narrowing down the search space by removing assignments that have illegal values. In particular, constraint propagation enforces constraints \textbf{locally} for certain variables in order to continue search.

\subsubsection{Arc-Consistency}

\begin{definition}
$X_i$ is \textbf{arc-consistent} with $X_j$ iff for every value $x_i \in D_i$, there exists some value $y_j \in D_j$ such that the binary constraint on the arc $(X_i, X_j)$ is satisfied
\end{definition}

That is, an arc $(X_i, X_j)$ is arc-consistent if there is a valid assignment for $X_j$ for every value that we assign to $X_i$. As we perform search down the tree, it is likely that most of the variables in the state that we are in or will be in, will not be arc-consistent with every other variable. If that is the case, then most likely whatever value we pick for the variable $X_i$, the leading assignment will violate some constraint since it is not arc-consistent. However, we can \textbf{enforce} arc-consistency by removing illegal values from the domain $D_i$ such that the resulting domain $D_{i}^{'}$ is arc-consistent with every other variable. Then, we have effectively reduced the domain $D_i$ for $X_i$ to one that contains legal values that do not lead to illegal assignments. This is done through:

\begin{itemize}
\item \textbf{Enforcing unary constraints} on the variable $X_i$. That is, we prune values from $D_i$ that do not enforce unary constraints $X_i \geq a$ for example.
\item \textbf{Propagating arc-consistency on binary constraints}. By enforcing arc-consistency on the arc $(X_i, X_j)$, then the arc-consistency on the neighbouring arc $(X_k, X_i)$  can be enforced on $D_k$ on the reduced domain $D_{i}^{'}$. This could not have been possible without enforcing arc-consistency on $(X_i, X_j)$ since we could have enforced arc-consistency on $(X_k, X_i)$ with respect to illegal values in $D_i$. This represents transitive arc-consistency relations.
\end{itemize}

Hence, arc-consistency acts as a form of local repeated constraint propagation by enforcing it after every assignment. This way, we prune as much as of the domain space as possible by removing values that will possibly lead to illegal assignments.

\begin{algorithm}
\caption{Arc-Consistency Algorithm AC-3}\label{euclid}
\begin{algorithmic}[1]
\Procedure{AC-3}{$csp$} \textbf{returns} $false$ if inconsistency found and $true$ otherwise
\State $queue \leftarrow$ all arcs in $csp$
\While{$queue$ is not empty}
$(X_i, X_j) \leftarrow$ Remove-First($queue$)
\If{Revise($csp, X_i, X_j$)}
\If{size of $D_i$ = 0} \Return $false$ \EndIf
\For{each $X_k \in$ $X_i$.neighbours -$\{X_j\}$}
\State add $(X_i, X_j)$ to $queue$
\EndFor
\EndIf
\EndWhile
\EndProcedure
\\
\Procedure{Revise}{$csp$} \textbf{returns} $true$ if the domain $D_i$ of $X_i$ is revised
\State $revised \leftarrow false$ 
\For{each $x \in D_i$}
\If{no value $y_j \in D_j$ allows $(x_i, y_j)$ to satisfy the constraint between $X_i$ and $X_j$}
\State delete $x$ from $D_i$
\State $revised \leftarrow true$
\EndIf
\EndFor 
\EndProcedure
\end{algorithmic}
\end{algorithm}

The AC-3 algorithm is one that allows us to ensure arc-consistency via constraint propagation. It first begins with arcs from the CSP problem, and prunes values from each domain $D_i$ for the variable $X_i$ if there are any values such that $X_i$ is not arc consistent with any $X_j$. Once we do this, we re-enforce transitive arc-consistency by re-evaluating the values in the domains of the other neighbours of $X_i$, $X_k$ such that once $(X_i, X_j)$ is arc-consistent after the Revise procedure, each of the other neighbours of $X_i, X_k$ needs to be arc-consistent with the revised domain of $X_i, D_{i}^{'}$.\\

Notice that for a CSP problem, there are at most $n^2$ directed arcs in a complete graph where very node has a directed edge to other nodes. Additionally, each $X_i$ has at most $d$ values in their respective domains $D_i$, and hence on revising of neighbouring arcs, an arc $(X_i, X_j)$ can be re-queued for the Revise operation at most $d$ times(exactly $d$ if everytime we only delete 1 element from the $d$ elements in $D_i$). Each revise operation compares each cartesian product of the domains $D_i, D_j$ and hence incurs at most $d^2$ comparisons. Putting all of these together, we have that the AC-3 algorithm incurs a total time complexity of $O(n^2d^3)$.\\

Hence, we can use the AC-3 algorithm as a routine on our CSP to ensure that all values in each domain is arc-consistent since it enforces transitive constraint propagation. In particular, we can re-execute this routine after every assignment to maintain the arc-consistency, and this entire process is known as \textbf{Maintaining Arc-Consistency} procedure. This ensures that after every assignment, we revise our domains to contains values that only lead to legal assignments.

\end{document}